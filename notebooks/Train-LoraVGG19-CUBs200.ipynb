{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "790ea580-9210-4cdc-979e-f770141273a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Adiciona o diretório pai ao caminho de pesquisa de módulos\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c779e9ab-0708-4301-a04d-8251ed874179",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import torchvision.models as models\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import gc\n",
    "import random\n",
    "import pickle\n",
    "from copy import deepcopy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b16827df-2d87-4963-b117-ed588eebdbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "from tqdm import tqdm\n",
    "from avalanche.benchmarks.classic import SplitCUB200\n",
    "from utils import create_plot_images\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from avalanche.evaluation.metrics import ConfusionMatrix, WandBStreamConfusionMatrix\n",
    "from avalanche.evaluation.metric_utils import default_cm_image_creator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b91f454-f413-4706-8fe4-839b1eef4b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((128, 128)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "eval_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((128, 128)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11978dfc-f109-487a-8b6e-ee65b0b6f342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Start of task  0 experience.current_experience 0\n",
      "Classes in this task: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]\n",
      "Task 0\n",
      "This task contains 3000 training examples\n",
      "This task contains 2864 test examples\n",
      "--------------------------------------------------------------------------------\n",
      "Start of task  0 experience.current_experience 1\n",
      "Classes in this task: [100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124]\n",
      "Task 0\n",
      "This task contains 749 training examples\n",
      "This task contains 714 test examples\n",
      "--------------------------------------------------------------------------------\n",
      "Start of task  0 experience.current_experience 2\n",
      "Classes in this task: [128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 125, 126, 127]\n",
      "Task 0\n",
      "This task contains 747 training examples\n",
      "This task contains 748 test examples\n",
      "--------------------------------------------------------------------------------\n",
      "Start of task  0 experience.current_experience 3\n",
      "Classes in this task: [150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174]\n",
      "Task 0\n",
      "This task contains 750 training examples\n",
      "This task contains 735 test examples\n",
      "--------------------------------------------------------------------------------\n",
      "Start of task  0 experience.current_experience 4\n",
      "Classes in this task: [175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199]\n",
      "Task 0\n",
      "This task contains 748 training examples\n",
      "This task contains 733 test examples\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                             | 0/334 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 3, 128, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'cub-200'\n",
    "total_number_classes = 200\n",
    "n_split_experiences = 5\n",
    "\n",
    "# creating the benchmark (scenario object)\n",
    "split_ds = SplitCUB200(\n",
    "    n_experiences=n_split_experiences,\n",
    "    seed=1234,\n",
    "    train_transform=train_transform,\n",
    "    eval_transform=eval_transform\n",
    "    \n",
    ")\n",
    "\n",
    "train_stream = split_ds.train_stream\n",
    "test_stream = split_ds.test_stream\n",
    "\n",
    "train_ds = []\n",
    "test_ds = []\n",
    "masks = []\n",
    "classes_in_experience = []\n",
    "\n",
    "for experience in train_stream:\n",
    "    print(\"Start of task \", experience.task_label, 'experience.current_experience', experience.current_experience)\n",
    "    print('Classes in this task:', experience.classes_in_this_experience)\n",
    "    masks.append([i for i in range(total_number_classes) if i not in experience.classes_in_this_experience])\n",
    "    classes_in_experience.append(experience.classes_in_this_experience)\n",
    "    \n",
    "    # The current Pytorch training set can be easily recovered through the \n",
    "    # experience\n",
    "    current_training_set = experience.dataset\n",
    "    train_ds.append(current_training_set)\n",
    "    \n",
    "    # ...as well as the task_label\n",
    "    print('Task {}'.format(experience.task_label))\n",
    "    print('This task contains', len(current_training_set), 'training examples')\n",
    "\n",
    "    # we can recover the corresponding test experience in the test stream\n",
    "    current_test_set = test_stream[experience.current_experience].dataset\n",
    "    print('This task contains', len(current_test_set), 'test examples')\n",
    "    test_ds.append(current_test_set)\n",
    "    print(80*'-')\n",
    "\n",
    "# Getting a Sample from the first Task\n",
    "batch_gen = torch.utils.data.DataLoader(train_ds[0], \n",
    "                              batch_size=9, \n",
    "                              shuffle=True, \n",
    "                              num_workers=1,\n",
    "                              )\n",
    "for batch in tqdm(batch_gen):\n",
    "    sample = batch[0]\n",
    "    target = batch[1]\n",
    "    print(sample.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49675386-9fb6-4916-95f6-63a10fb1bb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from continuous_lora.models.lora_vgg19 import LoraVGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95d184ed-cb66-4d82-8639-60e285dcbbf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/everton_aleixo/Documents/effects-of-lora-on-catastrophic-forgetting/venv/lib/python3.12/site-packages/torch/cuda/__init__.py:128: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8f3b88c-ca55-4cf7-972b-30dd237af3ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33meverton\u001b[0m (\u001b[33mcf-lora\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.18.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/everton_aleixo/Documents/effects-of-lora-on-catastrophic-forgetting/notebooks/wandb/run-20241005_120751-202vylsh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cf-lora/cf-lora-cub-200-loravgg-parameters/runs/202vylsh' target=\"_blank\">lora-vgg19-lr-00001-5tasks-lora-0</a></strong> to <a href='https://wandb.ai/cf-lora/cf-lora-cub-200-loravgg-parameters' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cf-lora/cf-lora-cub-200-loravgg-parameters' target=\"_blank\">https://wandb.ai/cf-lora/cf-lora-cub-200-loravgg-parameters</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cf-lora/cf-lora-cub-200-loravgg-parameters/runs/202vylsh' target=\"_blank\">https://wandb.ai/cf-lora/cf-lora-cub-200-loravgg-parameters/runs/202vylsh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/everton_aleixo/Documents/effects-of-lora-on-catastrophic-forgetting/venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------- TASK_0 ----------------------------------------\n",
      "Training task 0 in epoch 0. Batch size: 8.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                   | 0/375 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 93\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# Backpropagation and optimization\u001b[39;00m\n\u001b[1;32m     92\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 93\u001b[0m \u001b[43mloss_training\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     96\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_training\n",
      "File \u001b[0;32m~/Documents/effects-of-lora-on-catastrophic-forgetting/venv/lib/python3.12/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/effects-of-lora-on-catastrophic-forgetting/venv/lib/python3.12/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/effects-of-lora-on-catastrophic-forgetting/venv/lib/python3.12/site-packages/torch/autograd/graph.py:768\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    766\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tasks = [i for i in range(n_split_experiences)]\n",
    "acc_by_task = {i: 0 for i in range(n_split_experiences)}\n",
    "results_diff_models = {}\n",
    "best_model = {}\n",
    "cms = {}\n",
    "patience = 10\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "for loop in range(5):\n",
    "    #loop=1\n",
    "    if os.path.isfile('stop.txt'):\n",
    "        print(f'You should continue from loop {loop}')\n",
    "        break\n",
    "\n",
    "    config = {\n",
    "        \"learning_rate\": 0.00001,\n",
    "        \"weight_decay\": 5.e-4,\n",
    "        \"batch_size\": 8,\n",
    "        \"optimizer\": \"Adam\",\n",
    "        \"architecture\": \"LoraVGG19\",\n",
    "        \"dataset\": f'{dataset_name.upper()}',\n",
    "        \"epochs\": 300,\n",
    "        \"lr_schedule\": \"ReduceLROnPlateau - Patience 10 - Monitoring Val Accuracy\",\n",
    "        \"description\": \"Testing CUBs200 splited into 5 tasks.\"\n",
    "    }\n",
    "    wandb.init(\n",
    "        # set the wandb project where this run will be logged\n",
    "        project=f\"cf-lora-{dataset_name}-loravgg-parameters\",\n",
    "        name=f\"lora-vgg19-lr-00001-5tasks-lora-{loop}\",\n",
    "        \n",
    "        # track hyperparameters and run metadata\n",
    "        config=config\n",
    "    )\n",
    "    \n",
    "    for model_type in ['vgg19']:\n",
    "        bz=config['batch_size']\n",
    "            \n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.manual_seed(0)\n",
    "        random.seed(0)\n",
    "        np.random.seed(0)\n",
    "        base_model = models.vgg19_bn(weights=\"IMAGENET1K_V1\")\n",
    "        base_model.classifier[6] = nn.Linear(4096, total_number_classes)\n",
    "        model = LoraVGG19(\n",
    "            model=base_model,\n",
    "            masks=masks,\n",
    "            adapt_last_n_conv=16,\n",
    "            adapt_last_n_linear=3\n",
    "        )\n",
    "        \n",
    "                       \n",
    "        for task in tasks:\n",
    "            \n",
    "            epoch_losses = {'train_acc': [], 'train_loss': [], 'test_acc': [], 'test_loss': []}\n",
    "            model.to(device)\n",
    "            print(40*'-', f'TASK_{task}', 40*'-')\n",
    "        \n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate'], weight_decay=config['weight_decay'])\n",
    "            lr_scheduler = ReduceLROnPlateau(optimizer, mode='max', patience=4, verbose=True)\n",
    "            \n",
    "            early_stop = 0\n",
    "            best_val = 0\n",
    "            for i in range(config['epochs']):\n",
    "                g = torch.Generator()\n",
    "                g.manual_seed(0)\n",
    "                batch_gen = torch.utils.data.DataLoader(train_ds[task], \n",
    "                                                      batch_size=config['batch_size'], \n",
    "                                                      shuffle=True, \n",
    "                                                      num_workers=1,\n",
    "                                                      )\n",
    "                print(f'Training task {task} in epoch {i}. Batch size: {bz}.')\n",
    "                count = 0\n",
    "                total_loss = .0\n",
    "                total_hit = 0\n",
    "                for batch in tqdm(batch_gen):\n",
    "                    sample = batch[0].to(device)\n",
    "                    target = batch[1]\n",
    "                    target_onehot = F.one_hot(target, num_classes=total_number_classes).to(torch.float).to(device)\n",
    "    \n",
    "                    y_hat = model(sample)\n",
    "        \n",
    "                    # Compute the loss\n",
    "                    loss_training = criterion(y_hat, target_onehot)            \n",
    "                    \n",
    "                    # Backpropagation and optimization\n",
    "                    optimizer.zero_grad()\n",
    "                    loss_training.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                    total_loss += loss_training\n",
    "                    total_hit += sum(np.argmax(y_hat.cpu().detach().numpy(), axis=1) == target.numpy())\n",
    "    \n",
    "                    sample.to('cpu')\n",
    "                    target_onehot.to('cpu')\n",
    "                    del sample, target_onehot\n",
    "                    gc.collect()\n",
    "                    torch.cuda.empty_cache()\n",
    "                    \n",
    "                # Evaluate in test DS after each epoch\n",
    "                with torch.no_grad():\n",
    "                    batch_gen_test = torch.utils.data.DataLoader(test_ds[task], \n",
    "                                                      batch_size=config['batch_size'], \n",
    "                                                      shuffle=True, \n",
    "                                                      num_workers=1,\n",
    "                                                      )\n",
    "                    model.eval()\n",
    "                    test_loss = 0.\n",
    "                    acc = 0.\n",
    "                    for batch_test in tqdm(batch_gen_test):\n",
    "                        sample = batch_test[0].to(device)\n",
    "                        target = batch_test[1]\n",
    "                        target_onehot = F.one_hot(target, num_classes=total_number_classes).to(torch.float).to(device)\n",
    "            \n",
    "                        y_hat = model(sample)\n",
    "                        cpu_inference = y_hat.argmax(axis=1)\n",
    "                        \n",
    "                        test_loss += criterion(y_hat, target_onehot)\n",
    "                        acc += sum(cpu_inference.cpu() == target).item()\n",
    "    \n",
    "                        sample.to('cpu')\n",
    "                        target_onehot.to('cpu')\n",
    "                        del sample, target_onehot\n",
    "                        gc.collect()\n",
    "                        torch.cuda.empty_cache()\n",
    "    \n",
    "                    test_loss = (test_loss/len(batch_gen_test)).cpu()\n",
    "                    acc = acc/len(test_ds[task])\n",
    "                    lr_scheduler.step(acc)\n",
    "                    model.train()\n",
    "                \n",
    "                epoch_losses['train_acc'].append(total_hit/(len(batch_gen)*bz))\n",
    "                epoch_losses['train_loss'].append((total_loss/len(batch_gen)).cpu().item())\n",
    "                \n",
    "                epoch_losses['test_acc'].append(acc)\n",
    "                epoch_losses['test_loss'].append(test_loss.item())\n",
    "        \n",
    "                # if acc > (best_val+0.01):\n",
    "                if acc > (best_val):\n",
    "                    model.to('cpu')\n",
    "                    best_model[f'{model_type}-{task}'] = deepcopy(model)\n",
    "                    model.to(device)\n",
    "                    best_val = acc\n",
    "                    early_stop = 0\n",
    "        \n",
    "                if early_stop > patience:\n",
    "                    break\n",
    "                \n",
    "                early_stop += 1\n",
    "    \n",
    "                wandb.log({\"train_acc\": (total_hit/(len(batch_gen)*bz)), \"train_loss\": (total_loss/len(batch_gen)), \n",
    "                           \"val_acc\": acc, \"val_loss\": test_loss, \"task\": (task+1),\n",
    "                           \"Accuracy\": acc, \"Loss\": test_loss, \n",
    "                           \"patience\": early_stop, \"best_val_acc\": best_val, \"learning_rate\": optimizer.param_groups[0]['lr']\n",
    "                          })\n",
    "                \n",
    "                print(f'Trainig acc: {total_hit/(len(batch_gen)*bz):.4}   //   Training loss: {(total_loss/len(batch_gen)):.4f}   //   Test acc: {acc:.4f}   //   Test loss: {test_loss:.4f}')\n",
    "                print(f'early_stop: {early_stop}  /   Best acc: {best_val}')\n",
    "                del batch_gen, batch_gen_test\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "    \n",
    "            model.to('cpu')\n",
    "            del model\n",
    "            model = best_model[f'{model_type}-{task}']\n",
    "            del best_model[f'{model_type}-{task}']\n",
    "    \n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            # del criterion, optimizer, lr_scheduler \n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            target_task = task + 1\n",
    "            if target_task < len(tasks): \n",
    "                model.change_to_task(target_task)\n",
    "    \n",
    "        wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36dbbd70-df14-42bc-8b39-ab2e6754d3c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2dbc13-e944-4d5d-a797-e18da2ffa6db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1a7e5f73-c6d9-4e9e-8872-0383dc69c19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"weight_decay\": 5.e-4,\n",
    "    \"batch_size\": 128,\n",
    "    \"optimizer\": \"Adam\",\n",
    "    \"architecture\": \"LoraVGG19\",\n",
    "    \"dataset\": f'{dataset_name.upper()}',\n",
    "    \"epochs\": 10,\n",
    "    \"lr_schedule\": \"ReduceLROnPlateau - Patience 10 - Monitoring Val Accuracy\",\n",
    "    \"description\": \"Testing CUBs200 splited into 5 tasks.\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d1744754-467b-44a5-b905-57b98ee01b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = False\n",
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "base_model = models.vgg19_bn(weights=\"IMAGENET1K_V1\")\n",
    "base_model.classifier[6] = nn.Linear(4096, total_number_classes)\n",
    "model = LoraVGG19(\n",
    "    model=base_model,\n",
    "    masks=masks,\n",
    "    adapt_last_n_conv=16,\n",
    "    adapt_last_n_linear=3\n",
    ")\n",
    "tasks = [i for i in range(n_split_experiences)]\n",
    "acc_by_task = {i: 0 for i in range(n_split_experiences)}\n",
    "results_diff_models = {}\n",
    "best_model = {}\n",
    "cms = {}\n",
    "patience = 10\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "403b4359-5853-474d-b07e-1396aea895fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('cuda',\n",
       " LoraVGG19(\n",
       "   (features): Sequential(\n",
       "     (0): ContinuousConvLoRALayer(\n",
       "       (conv): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "     )\n",
       "     (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (2): ReLU(inplace=True)\n",
       "     (3): ContinuousConvLoRALayer(\n",
       "       (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "     )\n",
       "     (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (5): ReLU(inplace=True)\n",
       "     (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "     (7): ContinuousConvLoRALayer(\n",
       "       (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "     )\n",
       "     (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (9): ReLU(inplace=True)\n",
       "     (10): ContinuousConvLoRALayer(\n",
       "       (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "     )\n",
       "     (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (12): ReLU(inplace=True)\n",
       "     (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "     (14): ContinuousConvLoRALayer(\n",
       "       (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "     )\n",
       "     (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (16): ReLU(inplace=True)\n",
       "     (17): ContinuousConvLoRALayer(\n",
       "       (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "     )\n",
       "     (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (19): ReLU(inplace=True)\n",
       "     (20): ContinuousConvLoRALayer(\n",
       "       (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "     )\n",
       "     (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (22): ReLU(inplace=True)\n",
       "     (23): ContinuousConvLoRALayer(\n",
       "       (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "     )\n",
       "     (24): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (25): ReLU(inplace=True)\n",
       "     (26): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "     (27): ContinuousConvLoRALayer(\n",
       "       (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "     )\n",
       "     (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (29): ReLU(inplace=True)\n",
       "     (30): ContinuousConvLoRALayer(\n",
       "       (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "     )\n",
       "     (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (32): ReLU(inplace=True)\n",
       "     (33): ContinuousConvLoRALayer(\n",
       "       (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "     )\n",
       "     (34): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (35): ReLU(inplace=True)\n",
       "     (36): ContinuousConvLoRALayer(\n",
       "       (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "     )\n",
       "     (37): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (38): ReLU(inplace=True)\n",
       "     (39): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "     (40): ContinuousConvLoRALayer(\n",
       "       (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "     )\n",
       "     (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (42): ReLU(inplace=True)\n",
       "     (43): ContinuousConvLoRALayer(\n",
       "       (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "     )\n",
       "     (44): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (45): ReLU(inplace=True)\n",
       "     (46): ContinuousConvLoRALayer(\n",
       "       (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "     )\n",
       "     (47): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (48): ReLU(inplace=True)\n",
       "     (49): ContinuousConvLoRALayer(\n",
       "       (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "     )\n",
       "     (50): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (51): ReLU(inplace=True)\n",
       "     (52): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "   )\n",
       "   (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "   (classifier): Sequential(\n",
       "     (0): ContinuousLinearLoRALayer(in_features=25088, out_features=4096, bias=True)\n",
       "     (1): ReLU(inplace=True)\n",
       "     (2): Dropout(p=0.5, inplace=False)\n",
       "     (3): ContinuousLinearLoRALayer(in_features=4096, out_features=4096, bias=True)\n",
       "     (4): ReLU(inplace=True)\n",
       "     (5): Dropout(p=0.5, inplace=False)\n",
       "     (6): ContinuousLinearLoRALayer(in_features=4096, out_features=200, bias=True)\n",
       "   )\n",
       "   (softmax): Softmax(dim=-1)\n",
       " ))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "07cd8020-5fa2-4481-81c4-3e90c21a8b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ContinuousConvLoRALayer(\n",
      "  (conv): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n",
      "\t lora_A True\n",
      "\t lora_B True\n",
      "\t conv.weight True\n",
      "\t conv.bias True\n",
      "BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "\t weight True\n",
      "\t bias True\n",
      "ContinuousConvLoRALayer(\n",
      "  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n",
      "\t lora_A True\n",
      "\t lora_B True\n",
      "\t conv.weight True\n",
      "\t conv.bias True\n",
      "BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "\t weight True\n",
      "\t bias True\n",
      "ContinuousConvLoRALayer(\n",
      "  (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n",
      "\t lora_A True\n",
      "\t lora_B True\n",
      "\t conv.weight True\n",
      "\t conv.bias True\n",
      "BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "\t weight True\n",
      "\t bias True\n",
      "ContinuousConvLoRALayer(\n",
      "  (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n",
      "\t lora_A True\n",
      "\t lora_B True\n",
      "\t conv.weight True\n",
      "\t conv.bias True\n",
      "BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "\t weight True\n",
      "\t bias True\n",
      "ContinuousConvLoRALayer(\n",
      "  (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n",
      "\t lora_A True\n",
      "\t lora_B True\n",
      "\t conv.weight True\n",
      "\t conv.bias True\n",
      "BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "\t weight True\n",
      "\t bias True\n",
      "ContinuousConvLoRALayer(\n",
      "  (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n",
      "\t lora_A True\n",
      "\t lora_B True\n",
      "\t conv.weight True\n",
      "\t conv.bias True\n",
      "BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "\t weight True\n",
      "\t bias True\n",
      "ContinuousConvLoRALayer(\n",
      "  (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n",
      "\t lora_A True\n",
      "\t lora_B True\n",
      "\t conv.weight True\n",
      "\t conv.bias True\n",
      "BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "\t weight True\n",
      "\t bias True\n",
      "ContinuousConvLoRALayer(\n",
      "  (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n",
      "\t lora_A True\n",
      "\t lora_B True\n",
      "\t conv.weight True\n",
      "\t conv.bias True\n",
      "BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "\t weight True\n",
      "\t bias True\n",
      "ContinuousConvLoRALayer(\n",
      "  (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n",
      "\t lora_A True\n",
      "\t lora_B True\n",
      "\t conv.weight True\n",
      "\t conv.bias True\n",
      "BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "\t weight True\n",
      "\t bias True\n",
      "ContinuousConvLoRALayer(\n",
      "  (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n",
      "\t lora_A True\n",
      "\t lora_B True\n",
      "\t conv.weight True\n",
      "\t conv.bias True\n",
      "BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "\t weight True\n",
      "\t bias True\n",
      "ContinuousConvLoRALayer(\n",
      "  (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n",
      "\t lora_A True\n",
      "\t lora_B True\n",
      "\t conv.weight True\n",
      "\t conv.bias True\n",
      "BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "\t weight True\n",
      "\t bias True\n",
      "ContinuousConvLoRALayer(\n",
      "  (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n",
      "\t lora_A True\n",
      "\t lora_B True\n",
      "\t conv.weight True\n",
      "\t conv.bias True\n",
      "BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "\t weight True\n",
      "\t bias True\n",
      "ContinuousConvLoRALayer(\n",
      "  (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n",
      "\t lora_A True\n",
      "\t lora_B True\n",
      "\t conv.weight True\n",
      "\t conv.bias True\n",
      "BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "\t weight True\n",
      "\t bias True\n",
      "ContinuousConvLoRALayer(\n",
      "  (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n",
      "\t lora_A True\n",
      "\t lora_B True\n",
      "\t conv.weight True\n",
      "\t conv.bias True\n",
      "BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "\t weight True\n",
      "\t bias True\n",
      "ContinuousConvLoRALayer(\n",
      "  (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n",
      "\t lora_A True\n",
      "\t lora_B True\n",
      "\t conv.weight True\n",
      "\t conv.bias True\n",
      "BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "\t weight True\n",
      "\t bias True\n",
      "ContinuousConvLoRALayer(\n",
      "  (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n",
      "\t lora_A True\n",
      "\t lora_B True\n",
      "\t conv.weight True\n",
      "\t conv.bias True\n",
      "BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "\t weight True\n",
      "\t bias True\n",
      "ContinuousLinearLoRALayer(in_features=25088, out_features=4096, bias=True)\n",
      "\t weight True\n",
      "\t bias True\n",
      "\t lora_A True\n",
      "\t lora_B True\n",
      "ContinuousLinearLoRALayer(in_features=4096, out_features=4096, bias=True)\n",
      "\t weight True\n",
      "\t bias True\n",
      "\t lora_A True\n",
      "\t lora_B True\n",
      "ContinuousLinearLoRALayer(in_features=4096, out_features=200, bias=True)\n",
      "\t weight True\n",
      "\t bias True\n",
      "\t lora_A True\n",
      "\t lora_B True\n"
     ]
    }
   ],
   "source": [
    "for layer in model.features:\n",
    "    has_grad = False\n",
    "    for name, params in layer.named_parameters():\n",
    "        params.requires_grad = True\n",
    "        if params.requires_grad:\n",
    "            has_grad = True\n",
    "\n",
    "    if has_grad:\n",
    "        print(layer)\n",
    "        for name, params in layer.named_parameters():\n",
    "            print('\\t', name, params.requires_grad)\n",
    "        \n",
    "\n",
    "for layer in model.classifier:\n",
    "    has_grad = False\n",
    "    for name, params in layer.named_parameters():\n",
    "        params.requires_grad = True\n",
    "        if params.requires_grad:\n",
    "            has_grad = True\n",
    "\n",
    "    if has_grad:\n",
    "        print(layer)\n",
    "        for name, params in layer.named_parameters():\n",
    "            print('\\t', name, params.requires_grad)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d61bf218-eec9-44e2-8fee-5b4dd7d454f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features.0.lora_A True\n",
      "features.0.lora_B True\n",
      "features.0.conv.weight False\n",
      "features.0.conv.bias False\n",
      "features.1.weight True\n",
      "features.1.bias True\n",
      "features.3.lora_A True\n",
      "features.3.lora_B True\n",
      "features.3.conv.weight False\n",
      "features.3.conv.bias False\n",
      "features.4.weight True\n",
      "features.4.bias True\n",
      "features.7.lora_A True\n",
      "features.7.lora_B True\n",
      "features.7.conv.weight False\n",
      "features.7.conv.bias False\n",
      "features.8.weight True\n",
      "features.8.bias True\n",
      "features.10.lora_A True\n",
      "features.10.lora_B True\n",
      "features.10.conv.weight False\n",
      "features.10.conv.bias False\n",
      "features.11.weight True\n",
      "features.11.bias True\n",
      "features.14.lora_A True\n",
      "features.14.lora_B True\n",
      "features.14.conv.weight False\n",
      "features.14.conv.bias False\n",
      "features.15.weight True\n",
      "features.15.bias True\n",
      "features.17.lora_A True\n",
      "features.17.lora_B True\n",
      "features.17.conv.weight False\n",
      "features.17.conv.bias False\n",
      "features.18.weight True\n",
      "features.18.bias True\n",
      "features.20.lora_A True\n",
      "features.20.lora_B True\n",
      "features.20.conv.weight False\n",
      "features.20.conv.bias False\n",
      "features.21.weight True\n",
      "features.21.bias True\n",
      "features.23.lora_A True\n",
      "features.23.lora_B True\n",
      "features.23.conv.weight False\n",
      "features.23.conv.bias False\n",
      "features.24.weight True\n",
      "features.24.bias True\n",
      "features.27.lora_A True\n",
      "features.27.lora_B True\n",
      "features.27.conv.weight False\n",
      "features.27.conv.bias False\n",
      "features.28.weight True\n",
      "features.28.bias True\n",
      "features.30.lora_A True\n",
      "features.30.lora_B True\n",
      "features.30.conv.weight False\n",
      "features.30.conv.bias False\n",
      "features.31.weight True\n",
      "features.31.bias True\n",
      "features.33.lora_A True\n",
      "features.33.lora_B True\n",
      "features.33.conv.weight False\n",
      "features.33.conv.bias False\n",
      "features.34.weight True\n",
      "features.34.bias True\n",
      "features.36.lora_A True\n",
      "features.36.lora_B True\n",
      "features.36.conv.weight False\n",
      "features.36.conv.bias False\n",
      "features.37.weight True\n",
      "features.37.bias True\n",
      "features.40.lora_A True\n",
      "features.40.lora_B True\n",
      "features.40.conv.weight False\n",
      "features.40.conv.bias False\n",
      "features.41.weight True\n",
      "features.41.bias True\n",
      "features.43.lora_A True\n",
      "features.43.lora_B True\n",
      "features.43.conv.weight False\n",
      "features.43.conv.bias False\n",
      "features.44.weight True\n",
      "features.44.bias True\n",
      "features.46.lora_A True\n",
      "features.46.lora_B True\n",
      "features.46.conv.weight False\n",
      "features.46.conv.bias False\n",
      "features.47.weight True\n",
      "features.47.bias True\n",
      "features.49.lora_A True\n",
      "features.49.lora_B True\n",
      "features.49.conv.weight False\n",
      "features.49.conv.bias False\n",
      "features.50.weight True\n",
      "features.50.bias True\n",
      "classifier.0.weight False\n",
      "classifier.0.bias False\n",
      "classifier.0.lora_A True\n",
      "classifier.0.lora_B True\n",
      "classifier.3.weight False\n",
      "classifier.3.bias False\n",
      "classifier.3.lora_A True\n",
      "classifier.3.lora_B True\n",
      "classifier.6.weight False\n",
      "classifier.6.bias False\n",
      "classifier.6.lora_A True\n",
      "classifier.6.lora_B True\n"
     ]
    }
   ],
   "source": [
    "for name , param in model.named_parameters():\n",
    "    print(name, param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "943b33af-961b-4f15-8f9e-49e63ac84d78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ff08dcd2-ca78-4533-89d7-5593b0500ed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------- TASK_0 ----------------------------------------\n",
      "Training task 0 in epoch 0. Batch size: 128.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 24/24 [00:46<00:00,  1.94s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 23/23 [00:15<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainig acc: 0.009766   //   Training loss: 5.2958   //   Test acc: 0.0112   //   Test loss: 5.2958\n",
      "early_stop: 1  /   Best acc: 0.0111731843575419\n",
      "Training task 0 in epoch 1. Batch size: 128.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 24/24 [00:49<00:00,  2.08s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 23/23 [00:16<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainig acc: 0.01172   //   Training loss: 5.2958   //   Test acc: 0.0115   //   Test loss: 5.2958\n",
      "early_stop: 1  /   Best acc: 0.011522346368715084\n",
      "Training task 0 in epoch 2. Batch size: 128.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 24/24 [00:48<00:00,  2.04s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 23/23 [00:16<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainig acc: 0.008789   //   Training loss: 5.2958   //   Test acc: 0.0115   //   Test loss: 5.2958\n",
      "early_stop: 2  /   Best acc: 0.011522346368715084\n",
      "Training task 0 in epoch 3. Batch size: 128.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 24/24 [00:48<00:00,  2.04s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 23/23 [00:15<00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainig acc: 0.01074   //   Training loss: 5.2958   //   Test acc: 0.0115   //   Test loss: 5.2958\n",
      "early_stop: 3  /   Best acc: 0.011522346368715084\n",
      "Training task 0 in epoch 4. Batch size: 128.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 24/24 [00:49<00:00,  2.08s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 23/23 [00:15<00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainig acc: 0.0127   //   Training loss: 5.2957   //   Test acc: 0.0115   //   Test loss: 5.2958\n",
      "early_stop: 4  /   Best acc: 0.011522346368715084\n",
      "Changing to task 1\n",
      "Changing to task 1\n",
      "Changing to task 1\n",
      "Changing to task 1\n",
      "Changing to task 1\n",
      "Changing to task 1\n",
      "Changing to task 1\n",
      "Changing to task 1\n",
      "Changing to task 1\n",
      "Changing to task 1\n",
      "Changing to task 1\n",
      "Changing to task 1\n",
      "Changing to task 1\n",
      "Changing to task 1\n",
      "Changing to task 1\n",
      "Changing to task 1\n",
      "Changing to task 1\n",
      "Changing to task 1\n",
      "Changing to task 1\n",
      "Change to task 1. Remember to set the new weights into to optmizier.\n",
      "---------------------------------------- TASK_1 ----------------------------------------\n",
      "Training task 1 in epoch 0. Batch size: 128.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:12<00:00,  2.11s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:04<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainig acc: 0.05078   //   Training loss: 5.2936   //   Test acc: 0.0518   //   Test loss: 5.2937\n",
      "early_stop: 1  /   Best acc: 0.05182072829131653\n",
      "Training task 1 in epoch 1. Batch size: 128.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:12<00:00,  2.10s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:04<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainig acc: 0.04036   //   Training loss: 5.2938   //   Test acc: 0.0546   //   Test loss: 5.2937\n",
      "early_stop: 1  /   Best acc: 0.0546218487394958\n",
      "Training task 1 in epoch 2. Batch size: 128.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:12<00:00,  2.11s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:04<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainig acc: 0.05078   //   Training loss: 5.2935   //   Test acc: 0.0546   //   Test loss: 5.2937\n",
      "early_stop: 2  /   Best acc: 0.0546218487394958\n",
      "Training task 1 in epoch 3. Batch size: 128.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:12<00:00,  2.09s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:04<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainig acc: 0.04427   //   Training loss: 5.2937   //   Test acc: 0.0546   //   Test loss: 5.2937\n",
      "early_stop: 3  /   Best acc: 0.0546218487394958\n",
      "Training task 1 in epoch 4. Batch size: 128.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:12<00:00,  2.17s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:04<00:00,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainig acc: 0.04167   //   Training loss: 5.2938   //   Test acc: 0.0532   //   Test loss: 5.2937\n",
      "early_stop: 4  /   Best acc: 0.0546218487394958\n",
      "Changing to task 2\n",
      "Changing to task 2\n",
      "Changing to task 2\n",
      "Changing to task 2\n",
      "Changing to task 2\n",
      "Changing to task 2\n",
      "Changing to task 2\n",
      "Changing to task 2\n",
      "Changing to task 2\n",
      "Changing to task 2\n",
      "Changing to task 2\n",
      "Changing to task 2\n",
      "Changing to task 2\n",
      "Changing to task 2\n",
      "Changing to task 2\n",
      "Changing to task 2\n",
      "Changing to task 2\n",
      "Changing to task 2\n",
      "Changing to task 2\n",
      "Change to task 2. Remember to set the new weights into to optmizier.\n",
      "---------------------------------------- TASK_2 ----------------------------------------\n",
      "Training task 2 in epoch 0. Batch size: 128.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:12<00:00,  2.12s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:04<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainig acc: 0.04036   //   Training loss: 5.2940   //   Test acc: 0.0294   //   Test loss: 5.2941\n",
      "early_stop: 1  /   Best acc: 0.029411764705882353\n",
      "Training task 2 in epoch 1. Batch size: 128.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:12<00:00,  2.09s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:04<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainig acc: 0.02474   //   Training loss: 5.2941   //   Test acc: 0.0321   //   Test loss: 5.2941\n",
      "early_stop: 1  /   Best acc: 0.03208556149732621\n",
      "Training task 2 in epoch 2. Batch size: 128.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:12<00:00,  2.12s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:04<00:00,  1.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainig acc: 0.03516   //   Training loss: 5.2941   //   Test acc: 0.0321   //   Test loss: 5.2941\n",
      "early_stop: 2  /   Best acc: 0.03208556149732621\n",
      "Training task 2 in epoch 3. Batch size: 128.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:12<00:00,  2.07s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:04<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainig acc: 0.03906   //   Training loss: 5.2941   //   Test acc: 0.0294   //   Test loss: 5.2941\n",
      "early_stop: 3  /   Best acc: 0.03208556149732621\n",
      "Training task 2 in epoch 4. Batch size: 128.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:12<00:00,  2.14s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:04<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainig acc: 0.02865   //   Training loss: 5.2942   //   Test acc: 0.0294   //   Test loss: 5.2941\n",
      "early_stop: 4  /   Best acc: 0.03208556149732621\n",
      "Changing to task 3\n",
      "Changing to task 3\n",
      "Changing to task 3\n",
      "Changing to task 3\n",
      "Changing to task 3\n",
      "Changing to task 3\n",
      "Changing to task 3\n",
      "Changing to task 3\n",
      "Changing to task 3\n",
      "Changing to task 3\n",
      "Changing to task 3\n",
      "Changing to task 3\n",
      "Changing to task 3\n",
      "Changing to task 3\n",
      "Changing to task 3\n",
      "Changing to task 3\n",
      "Changing to task 3\n",
      "Changing to task 3\n",
      "Changing to task 3\n",
      "Change to task 3. Remember to set the new weights into to optmizier.\n",
      "---------------------------------------- TASK_3 ----------------------------------------\n",
      "Training task 3 in epoch 0. Batch size: 128.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:12<00:00,  2.15s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:04<00:00,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainig acc: 0.04818   //   Training loss: 5.2936   //   Test acc: 0.0735   //   Test loss: 5.2936\n",
      "early_stop: 1  /   Best acc: 0.07346938775510205\n",
      "Training task 3 in epoch 1. Batch size: 128.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:12<00:00,  2.08s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:04<00:00,  1.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainig acc: 0.0599   //   Training loss: 5.2936   //   Test acc: 0.0639   //   Test loss: 5.2936\n",
      "early_stop: 2  /   Best acc: 0.07346938775510205\n",
      "Training task 3 in epoch 2. Batch size: 128.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|███████████████████████████████████████████████████████████████████                                                                   | 3/6 [00:09<00:09,  3.03s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 39\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Backpropagation and optimization\u001b[39;00m\n\u001b[1;32m     38\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 39\u001b[0m \u001b[43mloss_training\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     42\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_training\n",
      "File \u001b[0;32m~/Documents/effects-of-lora-on-catastrophic-forgetting/venv/lib/python3.12/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/effects-of-lora-on-catastrophic-forgetting/venv/lib/python3.12/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/effects-of-lora-on-catastrophic-forgetting/venv/lib/python3.12/site-packages/torch/autograd/graph.py:768\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    766\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "bz=config['batch_size']\n",
    "model_type='vgg19'\n",
    "# device='cpu'\n",
    "for task in tasks:\n",
    "    epoch_losses = {'train_acc': [], 'train_loss': [], 'test_acc': [], 'test_loss': []}\n",
    "    model.to(device)\n",
    "    print(40*'-', f'TASK_{task}', 40*'-')\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate'], weight_decay=config['weight_decay'])\n",
    "    lr_scheduler = ReduceLROnPlateau(optimizer, mode='max', patience=4, verbose=True)\n",
    "    \n",
    "    early_stop = 0\n",
    "    best_val = 0\n",
    "    for i in range(5):\n",
    "        g = torch.Generator()\n",
    "        g.manual_seed(0)\n",
    "        batch_gen = torch.utils.data.DataLoader(train_ds[task], \n",
    "                                              batch_size=config['batch_size'], \n",
    "                                              shuffle=True, \n",
    "                                              num_workers=1,\n",
    "                                              )\n",
    "        print(f'Training task {task} in epoch {i}. Batch size: {bz}.')\n",
    "        count = 0\n",
    "        total_loss = .0\n",
    "        total_hit = 0\n",
    "        for batch in tqdm(batch_gen):\n",
    "            sample = batch[0].to(device)\n",
    "            target = batch[1]\n",
    "            target_onehot = F.one_hot(target, num_classes=total_number_classes).to(torch.float).to(device)\n",
    "\n",
    "            y_hat = model(sample)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss_training = criterion(y_hat, target_onehot)            \n",
    "            \n",
    "            # Backpropagation and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss_training.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss_training\n",
    "            total_hit += sum(np.argmax(y_hat.cpu().detach().numpy(), axis=1) == target.numpy())\n",
    "\n",
    "            sample.to('cpu')\n",
    "            target_onehot.to('cpu')\n",
    "            del sample, target_onehot\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        # Evaluate in test DS after each epoch\n",
    "        with torch.no_grad():\n",
    "            batch_gen_test = torch.utils.data.DataLoader(test_ds[task], \n",
    "                                              batch_size=config['batch_size'], \n",
    "                                              shuffle=True, \n",
    "                                              num_workers=1,\n",
    "                                              )\n",
    "            model.eval()\n",
    "            test_loss = 0.\n",
    "            acc = 0.\n",
    "            for batch_test in tqdm(batch_gen_test):\n",
    "                sample = batch_test[0].to(device)\n",
    "                target = batch_test[1]\n",
    "                target_onehot = F.one_hot(target, num_classes=total_number_classes).to(torch.float).to(device)\n",
    "    \n",
    "                y_hat = model(sample)\n",
    "                cpu_inference = y_hat.argmax(axis=1)\n",
    "                \n",
    "                test_loss += criterion(y_hat, target_onehot)\n",
    "                acc += sum(cpu_inference.cpu() == target).item()\n",
    "\n",
    "                sample.to('cpu')\n",
    "                target_onehot.to('cpu')\n",
    "                del sample, target_onehot\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "            test_loss = (test_loss/len(batch_gen_test)).cpu()\n",
    "            acc = acc/len(test_ds[task])\n",
    "            lr_scheduler.step(acc)\n",
    "            model.train()\n",
    "        \n",
    "        epoch_losses['train_acc'].append(total_hit/(len(batch_gen)*bz))\n",
    "        epoch_losses['train_loss'].append((total_loss/len(batch_gen)).cpu().item())\n",
    "        \n",
    "        epoch_losses['test_acc'].append(acc)\n",
    "        epoch_losses['test_loss'].append(test_loss.item())\n",
    "\n",
    "        # if acc > (best_val+0.01):\n",
    "        if acc > (best_val):\n",
    "            model.to('cpu')\n",
    "            best_model[f'{model_type}-{task}'] = deepcopy(model)\n",
    "            model.to(device)\n",
    "            best_val = acc\n",
    "            early_stop = 0\n",
    "\n",
    "        if early_stop > patience:\n",
    "            break\n",
    "        \n",
    "        early_stop += 1\n",
    "        \n",
    "        print(f'Trainig acc: {total_hit/(len(batch_gen)*bz):.4}   //   Training loss: {(total_loss/len(batch_gen)):.4f}   //   Test acc: {acc:.4f}   //   Test loss: {test_loss:.4f}')\n",
    "        print(f'early_stop: {early_stop}  /   Best acc: {best_val}')\n",
    "        del batch_gen, batch_gen_test\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    model.to('cpu')\n",
    "    model = best_model[f'{model_type}-{task}']\n",
    "    del best_model[f'{model_type}-{task}']\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # del criterion, optimizer, lr_scheduler \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    target_task = task + 1\n",
    "    if target_task < len(tasks): \n",
    "        model.change_to_task(target_task)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b143695a-d963-443c-b734-04ca342785da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                             | 0/92 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m y_hat \u001b[38;5;241m=\u001b[39m model(sample)\n\u001b[1;32m     15\u001b[0m cpu_inference \u001b[38;5;241m=\u001b[39m y_hat\u001b[38;5;241m.\u001b[39margmax(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 17\u001b[0m \u001b[43mtest_loss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_hat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_onehot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m acc \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(cpu_inference\u001b[38;5;241m.\u001b[39mcpu() \u001b[38;5;241m==\u001b[39m target)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     20\u001b[0m sample\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "source": [
    "batch_gen_test = torch.utils.data.DataLoader(test_ds[task], \n",
    "                                              batch_size=config['batch_size'], \n",
    "                                              shuffle=True, \n",
    "                                              num_workers=1,\n",
    "                                              )\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "with torch.no_grad():\n",
    "    for batch_test in tqdm(batch_gen_test):\n",
    "        sample = batch_test[0].to(device)\n",
    "        target = batch_test[1]\n",
    "        target_onehot = F.one_hot(target, num_classes=total_number_classes).to(torch.float).to(device)\n",
    "    \n",
    "        y_hat = model(sample)\n",
    "        cpu_inference = y_hat.argmax(axis=1)\n",
    "        \n",
    "        test_loss += criterion(y_hat, target_onehot)\n",
    "        acc += sum(cpu_inference.cpu() == target).item()\n",
    "    \n",
    "        sample.to('cpu')\n",
    "        target_onehot.to('cpu')\n",
    "        del sample, target_onehot\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c9c2023f-e289-48b4-a05f-92ffea4488f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, flatten, Tensor\n",
    "\n",
    "y_hat = model.features(sample)\n",
    "y_hat = model.avgpool(y_hat)\n",
    "\n",
    "y_hat = flatten(y_hat, 1)\n",
    "\n",
    "y_hat = model.classifier(y_hat)\n",
    "\n",
    "# Mask code\n",
    "y_hat = model.softmax(y_hat)\n",
    "y_hat[:, model.masks[model.current_task]] = .0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "63bc093f-fdb7-4456-b027-bc83a9c1d2e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0062, 0.0042, 0.0049, 0.0021,\n",
       "        0.0038, 0.0025, 0.0049, 0.0039, 0.0018, 0.0073, 0.0048, 0.0040, 0.0036,\n",
       "        0.0029, 0.0035, 0.0080, 0.0051, 0.0072, 0.0055, 0.0045, 0.0044, 0.0061,\n",
       "        0.0049, 0.0093], device='cuda:0', grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b1f5a19c-4eac-4ceb-a00e-33ed64101e78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "175"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.current_task\n",
    "len(model.masks[model.current_task])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1ee53b-f1fa-43d3-8544-982d51580e21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27818b10-b922-428f-84b5-a975955cbeca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fdd2e1-47a1-4db0-906f-ef6185030920",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1b8a6011-60c3-461a-9d1f-e707db39c40b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1728"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "64*3*3*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6316f128-5f2d-4afc-887c-6c2873c946e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1809"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "192*9 + 9*9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "37d1e020-8e92-4bdf-9b76-d8f6cfee5e48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 3, 3, 3])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.features[0].conv.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "569d80e0-1804-4390-b10c-889bdc09218a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4096, 25088]),\n",
       " 102760448,\n",
       " torch.Size([100, 25088]),\n",
       " torch.Size([4096, 100]),\n",
       " 2918400,\n",
       " 0.0284000318877551)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_lin = 100\n",
    "lin_params = (r_lin*25088+r_lin*4096)\n",
    "model.classifier[0].weight.shape, 4096*25088, model.classifier[0].lora_A.shape,  model.classifier[0].lora_B.shape, lin_params, lin_params/(4096*25088)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7703ad74-57c2-4855-91ef-1460d7d3ea8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"learning_rate\": 0.00001,\n",
    "    \"weight_decay\": 5.e-4,\n",
    "    \"batch_size\": 8,\n",
    "    \"optimizer\": \"Adam\",\n",
    "    \"architecture\": \"LoraVGG19\",\n",
    "    \"dataset\": f'{dataset_name.upper()}',\n",
    "    \"epochs\": 10,\n",
    "    \"lr_schedule\": \"ReduceLROnPlateau - Patience 10 - Monitoring Val Accuracy\",\n",
    "    \"description\": \"Testing CUBs200 splited into 5 tasks.\"\n",
    "}\n",
    "\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "base_model = models.vgg19_bn(weights=\"IMAGENET1K_V1\")\n",
    "base_model.classifier[6] = nn.Linear(4096, total_number_classes)\n",
    "model = LoraVGG19(\n",
    "    model=base_model,\n",
    "    masks=masks,\n",
    "    r_conv=9,\n",
    "    r_linear=30,\n",
    "    adapt_last_n_conv=0,\n",
    "    adapt_last_n_linear=3\n",
    ")\n",
    "model.classifier[6] = nn.Linear(4096, total_number_classes)\n",
    "\n",
    "tasks = [i for i in range(n_split_experiences)]\n",
    "acc_by_task = {i: 0 for i in range(n_split_experiences)}\n",
    "results_diff_models = {}\n",
    "best_model = {}\n",
    "cms = {}\n",
    "patience = 10\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8cffea80-dc65-4c55-bb6c-00fcf83dda1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.classifier[6].requires_grad_()\n",
    "model.classifier[3].requires_grad_()\n",
    "model.classifier[0].requires_grad_()\n",
    "model.classifier[3].bias.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0c65abd-222a-4dc0-b837-73e2373a2f14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 μs, sys: 0 ns, total: 3 μs\n",
      "Wall time: 5.96 μs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LoraVGG19(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (12): ReLU(inplace=True)\n",
       "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (16): ReLU(inplace=True)\n",
       "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (19): ReLU(inplace=True)\n",
       "    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (24): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (27): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (32): ReLU(inplace=True)\n",
       "    (33): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (34): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (35): ReLU(inplace=True)\n",
       "    (36): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (37): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (38): ReLU(inplace=True)\n",
       "    (39): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (42): ReLU(inplace=True)\n",
       "    (43): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (44): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (45): ReLU(inplace=True)\n",
       "    (46): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (47): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (48): ReLU(inplace=True)\n",
       "    (49): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (50): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (51): ReLU(inplace=True)\n",
       "    (52): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): ContinuousLinearLoRALayer(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): ContinuousLinearLoRALayer(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=200, bias=True)\n",
       "  )\n",
       "  (softmax): Softmax(dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# model.features[0].lora_A, model.features[0].lora_B\n",
    "# model.features[0].a\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f131607-00df-44f2-9c48-e726dbbcde25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1940680)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.features[0].conv.weight\n",
    "model.count_features_trainable_params(), model.count_classifier_trainable_params()\n",
    "\n",
    "# model.classifier[3].count_trainable_parameters()\n",
    "# model.classifier[6].bias.requires_grad, model.classifier[6].weight.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0d19e727-655d-4b3c-80d7-d18bea617d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------- TASK_0 ----------------------------------------\n",
      "Training task 0 in epoch 0. Batch size: 8.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 375/375 [01:40<00:00,  3.75it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 358/358 [01:21<00:00,  4.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainig acc: 0.04633   //   Training loss: 5.2792   //   Test acc: 0.1561   //   Test loss: 5.2033\n",
      "early_stop: 1  /   Best acc: 0.1560754189944134\n",
      "Training task 0 in epoch 1. Batch size: 8.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 375/375 [01:42<00:00,  3.67it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 358/358 [01:20<00:00,  4.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainig acc: 0.1627   //   Training loss: 5.1851   //   Test acc: 0.2559   //   Test loss: 5.1029\n",
      "early_stop: 1  /   Best acc: 0.25593575418994413\n",
      "Training task 0 in epoch 2. Batch size: 8.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 375/375 [01:40<00:00,  3.75it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 358/358 [01:19<00:00,  4.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainig acc: 0.245   //   Training loss: 5.1078   //   Test acc: 0.3132   //   Test loss: 5.0416\n",
      "early_stop: 1  /   Best acc: 0.3131983240223464\n",
      "Training task 0 in epoch 3. Batch size: 8.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 375/375 [01:40<00:00,  3.74it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 358/358 [01:17<00:00,  4.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainig acc: 0.3027   //   Training loss: 5.0528   //   Test acc: 0.3488   //   Test loss: 5.0000\n",
      "early_stop: 1  /   Best acc: 0.34881284916201116\n",
      "Training task 0 in epoch 4. Batch size: 8.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 375/375 [01:38<00:00,  3.79it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 358/358 [01:14<00:00,  4.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainig acc: 0.3547   //   Training loss: 5.0049   //   Test acc: 0.3774   //   Test loss: 4.9703\n",
      "early_stop: 1  /   Best acc: 0.3774441340782123\n",
      "Training task 0 in epoch 5. Batch size: 8.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 375/375 [01:30<00:00,  4.14it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 358/358 [01:11<00:00,  5.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainig acc: 0.381   //   Training loss: 4.9764   //   Test acc: 0.3900   //   Test loss: 4.9539\n",
      "early_stop: 1  /   Best acc: 0.39001396648044695\n",
      "Training task 0 in epoch 6. Batch size: 8.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 375/375 [01:30<00:00,  4.15it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 358/358 [01:11<00:00,  4.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainig acc: 0.4103   //   Training loss: 4.9406   //   Test acc: 0.4270   //   Test loss: 4.9136\n",
      "early_stop: 1  /   Best acc: 0.4270251396648045\n",
      "Training task 0 in epoch 7. Batch size: 8.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 375/375 [01:30<00:00,  4.14it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 358/358 [01:11<00:00,  5.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainig acc: 0.436   //   Training loss: 4.9192   //   Test acc: 0.4333   //   Test loss: 4.9098\n",
      "early_stop: 1  /   Best acc: 0.4333100558659218\n",
      "Training task 0 in epoch 8. Batch size: 8.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 375/375 [01:30<00:00,  4.15it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 358/358 [01:11<00:00,  5.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainig acc: 0.4587   //   Training loss: 4.8937   //   Test acc: 0.4612   //   Test loss: 4.8856\n",
      "early_stop: 1  /   Best acc: 0.4612430167597765\n",
      "Training task 0 in epoch 9. Batch size: 8.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 375/375 [01:32<00:00,  4.07it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 358/358 [01:11<00:00,  5.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainig acc: 0.479   //   Training loss: 4.8747   //   Test acc: 0.4637   //   Test loss: 4.8806\n",
      "early_stop: 1  /   Best acc: 0.46368715083798884\n",
      "Training task 0 in epoch 10. Batch size: 8.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 375/375 [01:28<00:00,  4.25it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 358/358 [01:11<00:00,  5.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainig acc: 0.4927   //   Training loss: 4.8585   //   Test acc: 0.4756   //   Test loss: 4.8620\n",
      "early_stop: 1  /   Best acc: 0.4755586592178771\n",
      "Training task 0 in epoch 11. Batch size: 8.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 375/375 [01:32<00:00,  4.04it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 358/358 [01:13<00:00,  4.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainig acc: 0.506   //   Training loss: 4.8414   //   Test acc: 0.4909   //   Test loss: 4.8483\n",
      "early_stop: 1  /   Best acc: 0.4909217877094972\n",
      "Training task 0 in epoch 12. Batch size: 8.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 375/375 [01:35<00:00,  3.92it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 358/358 [01:19<00:00,  4.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainig acc: 0.5187   //   Training loss: 4.8277   //   Test acc: 0.4895   //   Test loss: 4.8515\n",
      "early_stop: 2  /   Best acc: 0.4909217877094972\n",
      "Training task 0 in epoch 13. Batch size: 8.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 375/375 [01:35<00:00,  3.95it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 358/358 [01:16<00:00,  4.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainig acc: 0.533   //   Training loss: 4.8091   //   Test acc: 0.4976   //   Test loss: 4.8417\n",
      "early_stop: 1  /   Best acc: 0.4975558659217877\n",
      "Training task 0 in epoch 14. Batch size: 8.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 375/375 [01:37<00:00,  3.86it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 358/358 [01:19<00:00,  4.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainig acc: 0.538   //   Training loss: 4.8068   //   Test acc: 0.5024   //   Test loss: 4.8367\n",
      "early_stop: 1  /   Best acc: 0.5024441340782123\n",
      "Training task 0 in epoch 15. Batch size: 8.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 375/375 [01:35<00:00,  3.94it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 358/358 [01:19<00:00,  4.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainig acc: 0.5527   //   Training loss: 4.7926   //   Test acc: 0.5070   //   Test loss: 4.8323\n",
      "early_stop: 1  /   Best acc: 0.5069832402234636\n",
      "Training task 0 in epoch 16. Batch size: 8.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 375/375 [01:37<00:00,  3.86it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 358/358 [01:16<00:00,  4.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainig acc: 0.5593   //   Training loss: 4.7837   //   Test acc: 0.5105   //   Test loss: 4.8276\n",
      "early_stop: 1  /   Best acc: 0.5104748603351955\n",
      "Training task 0 in epoch 17. Batch size: 8.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 375/375 [01:36<00:00,  3.90it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 358/358 [01:17<00:00,  4.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainig acc: 0.5683   //   Training loss: 4.7782   //   Test acc: 0.5105   //   Test loss: 4.8281\n",
      "early_stop: 2  /   Best acc: 0.5104748603351955\n",
      "Training task 0 in epoch 18. Batch size: 8.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 375/375 [01:36<00:00,  3.89it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 358/358 [01:16<00:00,  4.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainig acc: 0.5733   //   Training loss: 4.7683   //   Test acc: 0.5237   //   Test loss: 4.8153\n",
      "early_stop: 1  /   Best acc: 0.5237430167597765\n",
      "Training task 0 in epoch 19. Batch size: 8.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 375/375 [01:35<00:00,  3.91it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 358/358 [01:16<00:00,  4.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainig acc: 0.5823   //   Training loss: 4.7611   //   Test acc: 0.5147   //   Test loss: 4.8149\n",
      "early_stop: 2  /   Best acc: 0.5237430167597765\n",
      "Training task 0 in epoch 20. Batch size: 8.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 375/375 [01:38<00:00,  3.81it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 358/358 [01:15<00:00,  4.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainig acc: 0.5917   //   Training loss: 4.7528   //   Test acc: 0.5360   //   Test loss: 4.7985\n",
      "early_stop: 1  /   Best acc: 0.535963687150838\n",
      "Training task 0 in epoch 21. Batch size: 8.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 375/375 [01:37<00:00,  3.83it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 358/358 [01:17<00:00,  4.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainig acc: 0.6053   //   Training loss: 4.7396   //   Test acc: 0.5349   //   Test loss: 4.8021\n",
      "early_stop: 2  /   Best acc: 0.535963687150838\n",
      "Training task 0 in epoch 22. Batch size: 8.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 375/375 [01:35<00:00,  3.93it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 358/358 [01:14<00:00,  4.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainig acc: 0.6143   //   Training loss: 4.7305   //   Test acc: 0.5422   //   Test loss: 4.7942\n",
      "early_stop: 1  /   Best acc: 0.5422486033519553\n",
      "Training task 0 in epoch 23. Batch size: 8.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 375/375 [01:33<00:00,  4.00it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 358/358 [01:12<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainig acc: 0.6197   //   Training loss: 4.7219   //   Test acc: 0.5349   //   Test loss: 4.8002\n",
      "early_stop: 2  /   Best acc: 0.5422486033519553\n",
      "Training task 0 in epoch 24. Batch size: 8.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 375/375 [01:33<00:00,  4.01it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 358/358 [01:13<00:00,  4.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainig acc: 0.63   //   Training loss: 4.7121   //   Test acc: 0.5426   //   Test loss: 4.7964\n",
      "early_stop: 1  /   Best acc: 0.5425977653631285\n",
      "Training task 0 in epoch 25. Batch size: 8.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 375/375 [01:32<00:00,  4.05it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 358/358 [01:12<00:00,  4.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainig acc: 0.6407   //   Training loss: 4.7008   //   Test acc: 0.5454   //   Test loss: 4.7901\n",
      "early_stop: 1  /   Best acc: 0.5453910614525139\n",
      "Training task 0 in epoch 26. Batch size: 8.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 375/375 [01:32<00:00,  4.05it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 358/358 [01:14<00:00,  4.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainig acc: 0.6403   //   Training loss: 4.7006   //   Test acc: 0.5485   //   Test loss: 4.7898\n",
      "early_stop: 1  /   Best acc: 0.5485335195530726\n",
      "Training task 0 in epoch 27. Batch size: 8.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 375/375 [01:32<00:00,  4.06it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 358/358 [01:12<00:00,  4.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainig acc: 0.648   //   Training loss: 4.6949   //   Test acc: 0.5559   //   Test loss: 4.7852\n",
      "early_stop: 1  /   Best acc: 0.5558659217877095\n",
      "Training task 0 in epoch 28. Batch size: 8.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 375/375 [01:32<00:00,  4.04it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 358/358 [01:13<00:00,  4.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainig acc: 0.6573   //   Training loss: 4.6820   //   Test acc: 0.5622   //   Test loss: 4.7726\n",
      "early_stop: 1  /   Best acc: 0.5621508379888268\n",
      "Training task 0 in epoch 29. Batch size: 8.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 375/375 [01:33<00:00,  3.99it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 358/358 [01:13<00:00,  4.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainig acc: 0.6613   //   Training loss: 4.6785   //   Test acc: 0.5562   //   Test loss: 4.7808\n",
      "early_stop: 2  /   Best acc: 0.5621508379888268\n",
      "Training task 0 in epoch 30. Batch size: 8.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 375/375 [01:34<00:00,  3.97it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 358/358 [01:13<00:00,  4.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainig acc: 0.671   //   Training loss: 4.6694   //   Test acc: 0.5608   //   Test loss: 4.7752\n",
      "early_stop: 3  /   Best acc: 0.5621508379888268\n",
      "Training task 0 in epoch 31. Batch size: 8.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 375/375 [01:31<00:00,  4.11it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 358/358 [01:12<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainig acc: 0.6797   //   Training loss: 4.6621   //   Test acc: 0.5702   //   Test loss: 4.7662\n",
      "early_stop: 1  /   Best acc: 0.57018156424581\n",
      "Training task 0 in epoch 32. Batch size: 8.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 375/375 [01:35<00:00,  3.92it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 358/358 [01:15<00:00,  4.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainig acc: 0.6833   //   Training loss: 4.6567   //   Test acc: 0.5814   //   Test loss: 4.7633\n",
      "early_stop: 1  /   Best acc: 0.5813547486033519\n",
      "Training task 0 in epoch 33. Batch size: 8.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 375/375 [01:34<00:00,  3.97it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 358/358 [01:15<00:00,  4.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainig acc: 0.6857   //   Training loss: 4.6521   //   Test acc: 0.5702   //   Test loss: 4.7659\n",
      "early_stop: 2  /   Best acc: 0.5813547486033519\n",
      "Training task 0 in epoch 34. Batch size: 8.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 375/375 [01:35<00:00,  3.93it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 358/358 [01:13<00:00,  4.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainig acc: 0.702   //   Training loss: 4.6388   //   Test acc: 0.5800   //   Test loss: 4.7580\n",
      "early_stop: 3  /   Best acc: 0.5813547486033519\n",
      "Training task 0 in epoch 35. Batch size: 8.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 375/375 [01:32<00:00,  4.04it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 358/358 [01:15<00:00,  4.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainig acc: 0.705   //   Training loss: 4.6372   //   Test acc: 0.5807   //   Test loss: 4.7625\n",
      "early_stop: 4  /   Best acc: 0.5813547486033519\n",
      "Training task 0 in epoch 36. Batch size: 8.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 375/375 [01:33<00:00,  4.02it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 358/358 [01:12<00:00,  4.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainig acc: 0.702   //   Training loss: 4.6378   //   Test acc: 0.5848   //   Test loss: 4.7550\n",
      "early_stop: 1  /   Best acc: 0.5848463687150838\n",
      "Training task 0 in epoch 37. Batch size: 8.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 375/375 [01:33<00:00,  4.02it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 358/358 [01:13<00:00,  4.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainig acc: 0.71   //   Training loss: 4.6295   //   Test acc: 0.5873   //   Test loss: 4.7549\n",
      "early_stop: 1  /   Best acc: 0.5872905027932961\n",
      "Training task 0 in epoch 38. Batch size: 8.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 375/375 [01:34<00:00,  3.97it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 358/358 [01:13<00:00,  4.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainig acc: 0.7067   //   Training loss: 4.6286   //   Test acc: 0.5775   //   Test loss: 4.7650\n",
      "early_stop: 2  /   Best acc: 0.5872905027932961\n",
      "Training task 0 in epoch 39. Batch size: 8.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 375/375 [01:33<00:00,  4.03it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 358/358 [01:23<00:00,  4.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainig acc: 0.7077   //   Training loss: 4.6272   //   Test acc: 0.5691   //   Test loss: 4.7691\n",
      "early_stop: 3  /   Best acc: 0.5872905027932961\n",
      "Training task 0 in epoch 40. Batch size: 8.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 375/375 [01:48<00:00,  3.44it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 358/358 [01:29<00:00,  3.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainig acc: 0.7177   //   Training loss: 4.6204   //   Test acc: 0.5971   //   Test loss: 4.7415\n",
      "early_stop: 1  /   Best acc: 0.5970670391061452\n",
      "Training task 0 in epoch 41. Batch size: 8.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 375/375 [01:47<00:00,  3.48it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 358/358 [01:31<00:00,  3.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainig acc: 0.722   //   Training loss: 4.6171   //   Test acc: 0.5964   //   Test loss: 4.7429\n",
      "early_stop: 2  /   Best acc: 0.5970670391061452\n",
      "Training task 0 in epoch 42. Batch size: 8.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 375/375 [01:43<00:00,  3.61it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 358/358 [01:32<00:00,  3.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainig acc: 0.725   //   Training loss: 4.6119   //   Test acc: 0.5800   //   Test loss: 4.7556\n",
      "early_stop: 3  /   Best acc: 0.5970670391061452\n",
      "Training task 0 in epoch 43. Batch size: 8.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 375/375 [01:46<00:00,  3.52it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 358/358 [01:18<00:00,  4.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainig acc: 0.7337   //   Training loss: 4.6025   //   Test acc: 0.5978   //   Test loss: 4.7423\n",
      "early_stop: 1  /   Best acc: 0.5977653631284916\n",
      "Training task 0 in epoch 44. Batch size: 8.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 375/375 [01:41<00:00,  3.71it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 358/358 [01:19<00:00,  4.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainig acc: 0.731   //   Training loss: 4.6046   //   Test acc: 0.5974   //   Test loss: 4.7393\n",
      "early_stop: 2  /   Best acc: 0.5977653631284916\n",
      "Training task 0 in epoch 45. Batch size: 8.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 375/375 [01:38<00:00,  3.82it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 358/358 [01:20<00:00,  4.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainig acc: 0.7377   //   Training loss: 4.5965   //   Test acc: 0.5845   //   Test loss: 4.7548\n",
      "early_stop: 3  /   Best acc: 0.5977653631284916\n",
      "Training task 0 in epoch 46. Batch size: 8.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 375/375 [01:38<00:00,  3.80it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 358/358 [01:21<00:00,  4.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainig acc: 0.734   //   Training loss: 4.5999   //   Test acc: 0.5768   //   Test loss: 4.7585\n",
      "early_stop: 4  /   Best acc: 0.5977653631284916\n",
      "Training task 0 in epoch 47. Batch size: 8.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 375/375 [01:38<00:00,  3.80it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 358/358 [01:19<00:00,  4.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainig acc: 0.7467   //   Training loss: 4.5911   //   Test acc: 0.5992   //   Test loss: 4.7407\n",
      "early_stop: 1  /   Best acc: 0.5991620111731844\n",
      "Training task 0 in epoch 48. Batch size: 8.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 375/375 [01:39<00:00,  3.76it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 358/358 [01:18<00:00,  4.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainig acc: 0.7407   //   Training loss: 4.5907   //   Test acc: 0.5890   //   Test loss: 4.7477\n",
      "early_stop: 2  /   Best acc: 0.5991620111731844\n",
      "Training task 0 in epoch 49. Batch size: 8.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 375/375 [01:40<00:00,  3.75it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 358/358 [01:22<00:00,  4.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainig acc: 0.736   //   Training loss: 4.5928   //   Test acc: 0.5950   //   Test loss: 4.7416\n",
      "early_stop: 3  /   Best acc: 0.5991620111731844\n",
      "Training task 0 in epoch 50. Batch size: 8.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 375/375 [01:38<00:00,  3.79it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 358/358 [01:18<00:00,  4.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainig acc: 0.7463   //   Training loss: 4.5879   //   Test acc: 0.5943   //   Test loss: 4.7421\n",
      "early_stop: 4  /   Best acc: 0.5991620111731844\n",
      "Training task 0 in epoch 51. Batch size: 8.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 375/375 [01:43<00:00,  3.64it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 358/358 [01:23<00:00,  4.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainig acc: 0.7467   //   Training loss: 4.5855   //   Test acc: 0.6027   //   Test loss: 4.7360\n",
      "early_stop: 1  /   Best acc: 0.6026536312849162\n",
      "Training task 0 in epoch 52. Batch size: 8.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 375/375 [01:50<00:00,  3.38it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 358/358 [01:27<00:00,  4.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainig acc: 0.7453   //   Training loss: 4.5851   //   Test acc: 0.5838   //   Test loss: 4.7510\n",
      "early_stop: 2  /   Best acc: 0.6026536312849162\n",
      "Training task 0 in epoch 53. Batch size: 8.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 375/375 [01:47<00:00,  3.48it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 358/358 [01:28<00:00,  4.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainig acc: 0.75   //   Training loss: 4.5793   //   Test acc: 0.5918   //   Test loss: 4.7460\n",
      "early_stop: 3  /   Best acc: 0.6026536312849162\n",
      "Training task 0 in epoch 54. Batch size: 8.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 375/375 [01:49<00:00,  3.43it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 358/358 [01:27<00:00,  4.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainig acc: 0.751   //   Training loss: 4.5776   //   Test acc: 0.5995   //   Test loss: 4.7381\n",
      "early_stop: 4  /   Best acc: 0.6026536312849162\n",
      "Training task 0 in epoch 55. Batch size: 8.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 375/375 [01:47<00:00,  3.49it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 358/358 [01:25<00:00,  4.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainig acc: 0.7507   //   Training loss: 4.5783   //   Test acc: 0.5964   //   Test loss: 4.7442\n",
      "early_stop: 5  /   Best acc: 0.6026536312849162\n",
      "Training task 0 in epoch 56. Batch size: 8.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 375/375 [01:47<00:00,  3.48it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 358/358 [01:27<00:00,  4.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainig acc: 0.7543   //   Training loss: 4.5757   //   Test acc: 0.5922   //   Test loss: 4.7426\n",
      "early_stop: 6  /   Best acc: 0.6026536312849162\n",
      "Training task 0 in epoch 57. Batch size: 8.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 375/375 [01:47<00:00,  3.48it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 358/358 [01:27<00:00,  4.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainig acc: 0.7587   //   Training loss: 4.5668   //   Test acc: 0.5985   //   Test loss: 4.7419\n",
      "early_stop: 7  /   Best acc: 0.6026536312849162\n",
      "Training task 0 in epoch 58. Batch size: 8.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 375/375 [01:48<00:00,  3.45it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 358/358 [01:24<00:00,  4.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainig acc: 0.762   //   Training loss: 4.5649   //   Test acc: 0.5999   //   Test loss: 4.7388\n",
      "early_stop: 8  /   Best acc: 0.6026536312849162\n",
      "Training task 0 in epoch 59. Batch size: 8.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 375/375 [01:47<00:00,  3.49it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 358/358 [01:30<00:00,  3.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainig acc: 0.7573   //   Training loss: 4.5679   //   Test acc: 0.6037   //   Test loss: 4.7313\n",
      "early_stop: 1  /   Best acc: 0.6037011173184358\n",
      "Training task 0 in epoch 60. Batch size: 8.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 375/375 [01:50<00:00,  3.38it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 358/358 [01:28<00:00,  4.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainig acc: 0.758   //   Training loss: 4.5657   //   Test acc: 0.6051   //   Test loss: 4.7322\n",
      "early_stop: 1  /   Best acc: 0.6050977653631285\n",
      "Training task 0 in epoch 61. Batch size: 8.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 375/375 [01:52<00:00,  3.33it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 358/358 [01:30<00:00,  3.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainig acc: 0.7597   //   Training loss: 4.5651   //   Test acc: 0.5981   //   Test loss: 4.7374\n",
      "early_stop: 2  /   Best acc: 0.6050977653631285\n",
      "Training task 0 in epoch 62. Batch size: 8.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 375/375 [01:50<00:00,  3.40it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 358/358 [01:31<00:00,  3.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainig acc: 0.76   //   Training loss: 4.5635   //   Test acc: 0.6166   //   Test loss: 4.7246\n",
      "early_stop: 1  /   Best acc: 0.6166201117318436\n",
      "Training task 0 in epoch 63. Batch size: 8.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 375/375 [01:47<00:00,  3.50it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 358/358 [01:27<00:00,  4.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainig acc: 0.758   //   Training loss: 4.5657   //   Test acc: 0.5985   //   Test loss: 4.7432\n",
      "early_stop: 2  /   Best acc: 0.6166201117318436\n",
      "Training task 0 in epoch 64. Batch size: 8.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 375/375 [01:51<00:00,  3.36it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 358/358 [01:28<00:00,  4.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainig acc: 0.7587   //   Training loss: 4.5653   //   Test acc: 0.6107   //   Test loss: 4.7274\n",
      "early_stop: 3  /   Best acc: 0.6166201117318436\n",
      "Training task 0 in epoch 65. Batch size: 8.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 375/375 [01:50<00:00,  3.40it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 358/358 [01:14<00:00,  4.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainig acc: 0.758   //   Training loss: 4.5647   //   Test acc: 0.6117   //   Test loss: 4.7300\n",
      "early_stop: 4  /   Best acc: 0.6166201117318436\n",
      "Training task 0 in epoch 66. Batch size: 8.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 375/375 [01:32<00:00,  4.05it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 358/358 [01:13<00:00,  4.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainig acc: 0.7603   //   Training loss: 4.5619   //   Test acc: 0.6013   //   Test loss: 4.7334\n",
      "early_stop: 5  /   Best acc: 0.6166201117318436\n",
      "Training task 0 in epoch 67. Batch size: 8.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 375/375 [01:35<00:00,  3.92it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 358/358 [01:16<00:00,  4.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainig acc: 0.7587   //   Training loss: 4.5645   //   Test acc: 0.5992   //   Test loss: 4.7367\n",
      "early_stop: 6  /   Best acc: 0.6166201117318436\n",
      "Training task 0 in epoch 68. Batch size: 8.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 375/375 [01:32<00:00,  4.04it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 358/358 [01:14<00:00,  4.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainig acc: 0.7613   //   Training loss: 4.5624   //   Test acc: 0.6093   //   Test loss: 4.7237\n",
      "early_stop: 7  /   Best acc: 0.6166201117318436\n",
      "Training task 0 in epoch 69. Batch size: 8.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 375/375 [01:33<00:00,  4.01it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 358/358 [01:14<00:00,  4.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainig acc: 0.7623   //   Training loss: 4.5632   //   Test acc: 0.6027   //   Test loss: 4.7357\n",
      "early_stop: 8  /   Best acc: 0.6166201117318436\n",
      "Training task 0 in epoch 70. Batch size: 8.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 375/375 [01:33<00:00,  4.00it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 358/358 [01:15<00:00,  4.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainig acc: 0.7583   //   Training loss: 4.5626   //   Test acc: 0.6093   //   Test loss: 4.7268\n",
      "early_stop: 9  /   Best acc: 0.6166201117318436\n",
      "Training task 0 in epoch 71. Batch size: 8.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 375/375 [01:35<00:00,  3.95it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 358/358 [01:16<00:00,  4.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainig acc: 0.7603   //   Training loss: 4.5626   //   Test acc: 0.6054   //   Test loss: 4.7265\n",
      "early_stop: 10  /   Best acc: 0.6166201117318436\n",
      "Training task 0 in epoch 72. Batch size: 8.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 375/375 [01:37<00:00,  3.86it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 358/358 [01:17<00:00,  4.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainig acc: 0.7587   //   Training loss: 4.5638   //   Test acc: 0.6023   //   Test loss: 4.7340\n",
      "early_stop: 11  /   Best acc: 0.6166201117318436\n",
      "Training task 0 in epoch 73. Batch size: 8.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 375/375 [01:36<00:00,  3.87it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 358/358 [01:16<00:00,  4.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3h 34min 22s, sys: 4min 16s, total: 3h 38min 38s\n",
      "Wall time: 3h 39min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "task = 0\n",
    "bz=8\n",
    "model_type='convlora'\n",
    "epoch_losses = {'train_acc': [], 'train_loss': [], 'test_acc': [], 'test_loss': []}\n",
    "model.to(device)\n",
    "print(40*'-', f'TASK_{task}', 40*'-')\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate'], weight_decay=config['weight_decay'])\n",
    "lr_scheduler = ReduceLROnPlateau(optimizer, mode='max', patience=4, verbose=True)\n",
    "\n",
    "early_stop = 0\n",
    "best_val = 0\n",
    "for i in range(300):\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(0)\n",
    "    batch_gen = torch.utils.data.DataLoader(train_ds[task], \n",
    "                                          batch_size=config['batch_size'], \n",
    "                                          shuffle=True, \n",
    "                                          num_workers=1,\n",
    "                                          )\n",
    "    print(f'Training task {task} in epoch {i}. Batch size: {bz}.')\n",
    "    count = 0\n",
    "    total_loss = .0\n",
    "    total_hit = 0\n",
    "    for batch in tqdm(batch_gen):\n",
    "        sample = batch[0].to(device)\n",
    "        target = batch[1]\n",
    "        target_onehot = F.one_hot(target, num_classes=total_number_classes).to(torch.float).to(device)\n",
    "\n",
    "        y_hat = model(sample)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss_training = criterion(y_hat, target_onehot)            \n",
    "        \n",
    "        # Backpropagation and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss_training.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss_training\n",
    "        total_hit += sum(np.argmax(y_hat.cpu().detach().numpy(), axis=1) == target.numpy())\n",
    "\n",
    "        sample.to('cpu')\n",
    "        target_onehot.to('cpu')\n",
    "        del sample, target_onehot\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    # Evaluate in test DS after each epoch\n",
    "    with torch.no_grad():\n",
    "        batch_gen_test = torch.utils.data.DataLoader(test_ds[task], \n",
    "                                          batch_size=config['batch_size'], \n",
    "                                          shuffle=True, \n",
    "                                          num_workers=1,\n",
    "                                          )\n",
    "        model.eval()\n",
    "        test_loss = 0.\n",
    "        acc = 0.\n",
    "        for batch_test in tqdm(batch_gen_test):\n",
    "            sample = batch_test[0].to(device)\n",
    "            target = batch_test[1]\n",
    "            target_onehot = F.one_hot(target, num_classes=total_number_classes).to(torch.float).to(device)\n",
    "\n",
    "            y_hat = model(sample)\n",
    "            cpu_inference = y_hat.argmax(axis=1)\n",
    "            \n",
    "            test_loss += criterion(y_hat, target_onehot)\n",
    "            acc += sum(cpu_inference.cpu() == target).item()\n",
    "\n",
    "            sample.to('cpu')\n",
    "            target_onehot.to('cpu')\n",
    "            del sample, target_onehot\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        test_loss = (test_loss/len(batch_gen_test)).cpu()\n",
    "        acc = acc/len(test_ds[task])\n",
    "        lr_scheduler.step(acc)\n",
    "        model.train()\n",
    "    \n",
    "    epoch_losses['train_acc'].append(total_hit/(len(batch_gen)*bz))\n",
    "    epoch_losses['train_loss'].append((total_loss/len(batch_gen)).cpu().item())\n",
    "    \n",
    "    epoch_losses['test_acc'].append(acc)\n",
    "    epoch_losses['test_loss'].append(test_loss.item())\n",
    "\n",
    "    # if acc > (best_val+0.01):\n",
    "    if acc > (best_val):\n",
    "        # model.to('cpu')\n",
    "        # best_model[f'{model_type}-{task}'] = deepcopy(model)\n",
    "        # model.to(device)\n",
    "        best_val = acc\n",
    "        early_stop = 0\n",
    "\n",
    "    if early_stop > patience:\n",
    "        break\n",
    "    \n",
    "    early_stop += 1\n",
    "    \n",
    "    print(f'Trainig acc: {total_hit/(len(batch_gen)*bz):.4}   //   Training loss: {(total_loss/len(batch_gen)):.4f}   //   Test acc: {acc:.4f}   //   Test loss: {test_loss:.4f}')\n",
    "    print(f'early_stop: {early_stop}  /   Best acc: {best_val}')\n",
    "    del batch_gen, batch_gen_test\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# model.to('cpu')\n",
    "# model = best_model[f'{model_type}-{task}']\n",
    "# del best_model[f'{model_type}-{task}']\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# del criterion, optimizer, lr_scheduler \n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "target_task = task + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb45fad9-8dd6-4dd8-ac0c-f40d0c951ff0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoraVGG19(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2dLora(3, 64, kernel_size=3, stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv2dLora(64, 64, kernel_size=3, stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): Conv2dLora(64, 128, kernel_size=3, stride=(1, 1), padding=(1, 1))\n",
       "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2dLora(128, 128, kernel_size=3, stride=(1, 1), padding=(1, 1))\n",
       "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (12): ReLU(inplace=True)\n",
       "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (14): Conv2dLora(128, 256, kernel_size=3, stride=(1, 1), padding=(1, 1))\n",
       "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (16): ReLU(inplace=True)\n",
       "    (17): Conv2dLora(256, 256, kernel_size=3, stride=(1, 1), padding=(1, 1))\n",
       "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (19): ReLU(inplace=True)\n",
       "    (20): Conv2dLora(256, 256, kernel_size=3, stride=(1, 1), padding=(1, 1))\n",
       "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): Conv2dLora(256, 256, kernel_size=3, stride=(1, 1), padding=(1, 1))\n",
       "    (24): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (27): Conv2dLora(256, 512, kernel_size=3, stride=(1, 1), padding=(1, 1))\n",
       "    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): Conv2dLora(512, 512, kernel_size=3, stride=(1, 1), padding=(1, 1))\n",
       "    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (32): ReLU(inplace=True)\n",
       "    (33): Conv2dLora(512, 512, kernel_size=3, stride=(1, 1), padding=(1, 1))\n",
       "    (34): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (35): ReLU(inplace=True)\n",
       "    (36): Conv2dLora(512, 512, kernel_size=3, stride=(1, 1), padding=(1, 1))\n",
       "    (37): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (38): ReLU(inplace=True)\n",
       "    (39): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (40): Conv2dLora(512, 512, kernel_size=3, stride=(1, 1), padding=(1, 1))\n",
       "    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (42): ReLU(inplace=True)\n",
       "    (43): Conv2dLora(512, 512, kernel_size=3, stride=(1, 1), padding=(1, 1))\n",
       "    (44): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (45): ReLU(inplace=True)\n",
       "    (46): Conv2dLora(512, 512, kernel_size=3, stride=(1, 1), padding=(1, 1))\n",
       "    (47): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (48): ReLU(inplace=True)\n",
       "    (49): Conv2dLora(512, 512, kernel_size=3, stride=(1, 1), padding=(1, 1))\n",
       "    (50): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (51): ReLU(inplace=True)\n",
       "    (52): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=200, bias=True)\n",
       "  )\n",
       "  (softmax): Softmax(dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "949c93ad-ccef-401c-8ed5-5c4d3141c92c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([[-0.0378,  0.0567,  0.1117, -0.0339,  0.0553, -0.0040, -0.0299,  0.0211,\n",
       "          -0.0835],\n",
       "         [-0.0905,  0.1162,  0.0151, -0.0443, -0.0483,  0.1258, -0.0734,  0.1352,\n",
       "          -0.1183],\n",
       "         [-0.1429,  0.0847,  0.0833,  0.0792, -0.0334, -0.0474,  0.0179,  0.0147,\n",
       "          -0.0258],\n",
       "         [-0.0596, -0.1432,  0.0600,  0.0647,  0.1609, -0.1108, -0.0214,  0.1754,\n",
       "          -0.1787],\n",
       "         [ 0.1239, -0.0960, -0.0290,  0.0268, -0.0605,  0.1394, -0.0755, -0.0516,\n",
       "           0.0481],\n",
       "         [-0.0926, -0.0775, -0.0588, -0.0384,  0.1045,  0.0654,  0.0716, -0.0614,\n",
       "           0.0230],\n",
       "         [-0.0505,  0.0856,  0.1879, -0.0491, -0.2247,  0.0297,  0.1345, -0.0377,\n",
       "          -0.0010],\n",
       "         [-0.0702,  0.0286, -0.1730,  0.0926,  0.0122,  0.0460, -0.1009,  0.1071,\n",
       "          -0.0044],\n",
       "         [ 0.0469, -0.0567, -0.1212, -0.0473, -0.2061,  0.1177,  0.1110,  0.0067,\n",
       "           0.0723]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 2.1457e-27,  2.4783e-17, -4.2105e-29,  ..., -2.7441e-27,\n",
       "          -1.0097e-18, -2.3307e-26],\n",
       "         [-1.4450e-27,  1.0416e-18,  1.6854e-28,  ...,  8.0198e-28,\n",
       "           3.8833e-22,  1.9103e-27],\n",
       "         [-1.8321e-27,  7.5574e-17,  1.3306e-30,  ...,  2.4115e-27,\n",
       "          -2.3747e-17, -9.5535e-27],\n",
       "         ...,\n",
       "         [ 3.8652e-07,  3.7321e-04,  2.9757e-13,  ...,  1.3186e-05,\n",
       "          -2.9180e-08, -1.8536e-07],\n",
       "         [ 9.1776e-07,  2.5407e-04, -1.3945e-13,  ...,  2.0182e-05,\n",
       "          -6.9665e-09, -1.2397e-06],\n",
       "         [ 5.2322e-06, -1.7211e-04,  6.5537e-12,  ...,  4.7336e-05,\n",
       "           1.1530e-08, -6.1934e-06]], requires_grad=True))"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.features[0].lora_A, model.features[0].lora_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "688b1f81-fe4e-40ba-baaf-a12c64aae817",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[-3.2130e-08, -3.1717e-08, -6.7120e-08],\n",
       "          [-2.9321e-08, -2.1596e-08, -5.3207e-08],\n",
       "          [-5.3304e-08, -4.0832e-08, -6.6434e-08]],\n",
       "\n",
       "         [[ 1.3710e-07,  1.2848e-07,  7.1270e-08],\n",
       "          [ 1.5487e-07,  1.5186e-07,  8.3582e-08],\n",
       "          [ 9.3392e-08,  9.9451e-08,  4.1284e-08]],\n",
       "\n",
       "         [[ 1.7435e-07,  1.7840e-07,  1.0561e-07],\n",
       "          [ 1.9644e-07,  1.9769e-07,  1.2560e-07],\n",
       "          [ 1.1935e-07,  1.3447e-07,  5.9638e-08]]],\n",
       "\n",
       "\n",
       "        [[[-6.3467e-08, -5.2714e-08, -3.8511e-08],\n",
       "          [-1.4349e-08, -9.0426e-09, -1.7164e-08],\n",
       "          [ 2.2282e-08,  1.9084e-08,  1.8150e-09]],\n",
       "\n",
       "         [[-1.2191e-08, -6.1933e-09,  8.5022e-09],\n",
       "          [ 3.8618e-08,  5.8869e-08,  2.2082e-08],\n",
       "          [ 6.2501e-08,  7.0528e-08,  2.7185e-08]],\n",
       "\n",
       "         [[ 3.0045e-08,  3.8003e-08,  3.5389e-08],\n",
       "          [ 8.6743e-08,  1.1626e-07,  8.0502e-08],\n",
       "          [ 7.9653e-08,  1.0057e-07,  6.7894e-08]]],\n",
       "\n",
       "\n",
       "        [[[-1.3565e-01, -2.9960e-01, -1.5151e-01],\n",
       "          [-2.6804e-01, -3.0505e-01, -9.4600e-02],\n",
       "          [-1.4124e-01, -8.1448e-02,  1.7112e-01]],\n",
       "\n",
       "         [[ 2.0333e-01,  4.0123e-01,  1.3747e-01],\n",
       "          [ 4.2157e-01,  6.7595e-01,  3.2725e-01],\n",
       "          [ 1.3103e-01,  3.2654e-01,  6.4284e-02]],\n",
       "\n",
       "         [[-4.1092e-02,  4.6723e-02, -9.7236e-03],\n",
       "          [ 2.9558e-02,  3.5226e-02, -1.2338e-01],\n",
       "          [-9.1504e-02, -1.7836e-01, -3.1635e-01]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[-5.0743e-08, -6.6759e-08, -7.8941e-08],\n",
       "          [-4.6060e-08, -7.9228e-08, -7.3020e-08],\n",
       "          [-7.2354e-08, -7.5888e-08, -7.5602e-08]],\n",
       "\n",
       "         [[-1.8643e-08, -5.4891e-08, -5.5029e-08],\n",
       "          [-2.9716e-08, -6.0283e-08, -5.9039e-08],\n",
       "          [-5.4538e-08, -5.7847e-08, -5.4300e-08]],\n",
       "\n",
       "         [[-9.0202e-09, -4.0853e-08, -4.0702e-08],\n",
       "          [-2.0145e-08, -4.0434e-08, -4.7582e-08],\n",
       "          [-4.9317e-08, -4.9057e-08, -4.8955e-08]]],\n",
       "\n",
       "\n",
       "        [[[-7.8860e-09,  4.4172e-09,  4.1371e-08],\n",
       "          [-9.1843e-09,  4.9512e-09,  6.3214e-08],\n",
       "          [ 1.2759e-09,  2.6705e-08,  6.1890e-08]],\n",
       "\n",
       "         [[-1.6242e-09, -5.1753e-10,  3.5820e-08],\n",
       "          [-1.5325e-08,  9.5484e-09,  6.6321e-08],\n",
       "          [ 5.8824e-09,  3.6904e-08,  7.9135e-08]],\n",
       "\n",
       "         [[ 7.1371e-09,  3.5890e-09,  2.7016e-08],\n",
       "          [-2.0624e-09,  1.1004e-08,  5.9306e-08],\n",
       "          [ 2.0382e-08,  3.8491e-08,  6.5097e-08]]],\n",
       "\n",
       "\n",
       "        [[[-5.1288e-07,  4.4387e-06,  2.8007e-06],\n",
       "          [ 1.4936e-06,  6.4549e-06,  4.6987e-06],\n",
       "          [ 1.3737e-06,  3.8875e-06,  3.2473e-06]],\n",
       "\n",
       "         [[-2.3703e-06,  3.5919e-06,  1.5149e-06],\n",
       "          [ 6.3111e-07,  6.6647e-06,  4.2479e-06],\n",
       "          [-2.4147e-07,  2.5924e-06,  1.3272e-06]],\n",
       "\n",
       "         [[ 4.0485e-06,  1.1047e-05,  7.5509e-06],\n",
       "          [ 7.4615e-06,  1.4459e-05,  1.0637e-05],\n",
       "          [ 4.2098e-06,  7.7080e-06,  5.1014e-06]]]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.features[0].conv.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6546cad-97e5-4ac6-b072-69beaaf90d60",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1096729732.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[11], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    Trainig acc: 7.438   //   Training loss: 4.8364   //   Test acc: 0.4036   //   Test loss: 4.9151\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "3\n",
    "\n",
    "Trainig acc: 7.438   //   Training loss: 4.8364   //   Test acc: 0.4036   //   Test loss: 4.9151\n",
    "early_stop: 11  /   Best acc: 0.4057262569832402"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b318d07-e8ea-4166-b7e5-71322ad2001a",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (637401105.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[12], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    Trainig acc: 9.818   //   Training loss: 4.6811   //   Test acc: 0.5112   //   Test loss: 4.8094\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "30\n",
    "\n",
    "Trainig acc: 9.818   //   Training loss: 4.6811   //   Test acc: 0.5112   //   Test loss: 4.8094\n",
    "early_stop: 11  /   Best acc: 0.5115223463687151\n",
    "Training task 0 in epoch 81. Batch size: 8.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dca528ab-7adb-4a8e-8a63-2caed3bbb9de",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3970758156.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[11], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    Trainig acc: 0.973   //   Training loss: 4.3385   //   Test acc: 0.7678   //   Test loss: 4.5556\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "old -> Treinava todas as Conv Layers , nao usava a Lora\n",
    "\n",
    "Trainig acc: 0.973   //   Training loss: 4.3385   //   Test acc: 0.7678   //   Test loss: 4.5556\n",
    "early_stop: 11  /   Best acc: 0.7730446927374302\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4e93d0d-42d7-496e-affa-a17ac6b04973",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3907677468.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[12], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    Trainig acc: 0.9033   //   Training loss: 4.4146   //   Test acc: 0.7360   //   Test loss: 4.5894\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "ContinuousConvLora - R=3\n",
    "\n",
    "Trainig acc: 0.9033   //   Training loss: 4.4146   //   Test acc: 0.7360   //   Test loss: 4.5894\n",
    "early_stop: 11  /   Best acc: 0.7409217877094972\n",
    "Training task 0 in epoch 92. Batch size: 8.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c00361-7fa7-443b-bd21-5242d0d0fc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "old (changed) - Congelei as conv e treinei apenas as lora\n",
    "\n",
    "Trainig acc: 0.9207   //   Training loss: 4.3934   //   Test acc: 0.7497   //   Test loss: 4.5735\n",
    "early_stop: 11  /   Best acc: 0.7548882681564246\n",
    "Training task 0 in epoch 83. Batch size: 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0544abf7-1e49-4306-adf7-ce34e2c32e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "ContinuousConvLora - R=1\n",
    "\n",
    "Trainig acc: 0.784   //   Training loss: 4.5358   //   Test acc: 0.6568   //   Test loss: 4.6754\n",
    "early_stop: 11  /   Best acc: 0.6588687150837989"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c12850-90c7-4b5f-a9d6-f0f3afef26c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20f4371-8999-4a48-976c-b36fa5853ac4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0151e852-8dab-44cd-b079-ed66c072c2b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "in_channel: 3 | out_channel 64\n",
      "weight é um parametro treinável. 1728. torch.Size([64, 3, 3, 3])\n",
      "bias é um parametro treinável. 64. torch.Size([64])\n",
      "Numero de parametros treinaveis na layer Conv normal: 1792\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------Continuous Conv Lora\n",
      "lora_A é um parametro treinável. 675. torch.Size([75, 9])\n",
      "lora_B é um parametro treinável. 14400. torch.Size([192, 75])\n",
      "conv.weight NÃO é um parametro treinável\n",
      "conv.bias é um parametro treinável. 64. torch.Size([64])\n",
      "Numero de parametros treinaveis ConvLora terceiro: 15139\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------Continuous Conv Lora NOSSO\n",
      "weight NÃO é um parametro treinável\n",
      "bias NÃO é um parametro treinável\n",
      "a é um parametro treinável. 9. torch.Size([1, 3, 3, 1])\n",
      "b é um parametro treinável. 9. torch.Size([1, 3, 1, 3])\n",
      "c é um parametro treinável. 192. torch.Size([64, 3, 1, 1])\n",
      "d é um parametro treinável. 64. torch.Size([64])\n",
      "d_ é um parametro treinável. 1. torch.Size([1])\n",
      "Numero de parametros treinaveis no nosso ConvLora: 275\n",
      "\n",
      "\n",
      "\n",
      "in_channel: 64 | out_channel 64\n",
      "weight é um parametro treinável. 36864. torch.Size([64, 64, 3, 3])\n",
      "bias é um parametro treinável. 64. torch.Size([64])\n",
      "Numero de parametros treinaveis na layer Conv normal: 36928\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------Continuous Conv Lora\n",
      "lora_A é um parametro treinável. 14400. torch.Size([75, 192])\n",
      "lora_B é um parametro treinável. 14400. torch.Size([192, 75])\n",
      "conv.weight NÃO é um parametro treinável\n",
      "conv.bias é um parametro treinável. 64. torch.Size([64])\n",
      "Numero de parametros treinaveis ConvLora terceiro: 28864\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------Continuous Conv Lora NOSSO\n",
      "weight NÃO é um parametro treinável\n",
      "bias NÃO é um parametro treinável\n",
      "a é um parametro treinável. 192. torch.Size([1, 64, 3, 1])\n",
      "b é um parametro treinável. 192. torch.Size([1, 64, 1, 3])\n",
      "c é um parametro treinável. 4096. torch.Size([64, 64, 1, 1])\n",
      "d é um parametro treinável. 64. torch.Size([64])\n",
      "d_ é um parametro treinável. 1. torch.Size([1])\n",
      "Numero de parametros treinaveis no nosso ConvLora: 4545\n",
      "\n",
      "\n",
      "\n",
      "in_channel: 64 | out_channel 128\n",
      "weight é um parametro treinável. 73728. torch.Size([128, 64, 3, 3])\n",
      "bias é um parametro treinável. 128. torch.Size([128])\n",
      "Numero de parametros treinaveis na layer Conv normal: 73856\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------Continuous Conv Lora\n",
      "lora_A é um parametro treinável. 14400. torch.Size([75, 192])\n",
      "lora_B é um parametro treinável. 28800. torch.Size([384, 75])\n",
      "conv.weight NÃO é um parametro treinável\n",
      "conv.bias é um parametro treinável. 128. torch.Size([128])\n",
      "Numero de parametros treinaveis ConvLora terceiro: 43328\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------Continuous Conv Lora NOSSO\n",
      "weight NÃO é um parametro treinável\n",
      "bias NÃO é um parametro treinável\n",
      "a é um parametro treinável. 192. torch.Size([1, 64, 3, 1])\n",
      "b é um parametro treinável. 192. torch.Size([1, 64, 1, 3])\n",
      "c é um parametro treinável. 8192. torch.Size([128, 64, 1, 1])\n",
      "d é um parametro treinável. 128. torch.Size([128])\n",
      "d_ é um parametro treinável. 1. torch.Size([1])\n",
      "Numero de parametros treinaveis no nosso ConvLora: 8705\n",
      "\n",
      "\n",
      "\n",
      "in_channel: 128 | out_channel 128\n",
      "weight é um parametro treinável. 147456. torch.Size([128, 128, 3, 3])\n",
      "bias é um parametro treinável. 128. torch.Size([128])\n",
      "Numero de parametros treinaveis na layer Conv normal: 147584\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------Continuous Conv Lora\n",
      "lora_A é um parametro treinável. 28800. torch.Size([75, 384])\n",
      "lora_B é um parametro treinável. 28800. torch.Size([384, 75])\n",
      "conv.weight NÃO é um parametro treinável\n",
      "conv.bias é um parametro treinável. 128. torch.Size([128])\n",
      "Numero de parametros treinaveis ConvLora terceiro: 57728\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------Continuous Conv Lora NOSSO\n",
      "weight NÃO é um parametro treinável\n",
      "bias NÃO é um parametro treinável\n",
      "a é um parametro treinável. 384. torch.Size([1, 128, 3, 1])\n",
      "b é um parametro treinável. 384. torch.Size([1, 128, 1, 3])\n",
      "c é um parametro treinável. 16384. torch.Size([128, 128, 1, 1])\n",
      "d é um parametro treinável. 128. torch.Size([128])\n",
      "d_ é um parametro treinável. 1. torch.Size([1])\n",
      "Numero de parametros treinaveis no nosso ConvLora: 17281\n",
      "\n",
      "\n",
      "\n",
      "in_channel: 128 | out_channel 256\n",
      "weight é um parametro treinável. 294912. torch.Size([256, 128, 3, 3])\n",
      "bias é um parametro treinável. 256. torch.Size([256])\n",
      "Numero de parametros treinaveis na layer Conv normal: 295168\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------Continuous Conv Lora\n",
      "lora_A é um parametro treinável. 28800. torch.Size([75, 384])\n",
      "lora_B é um parametro treinável. 57600. torch.Size([768, 75])\n",
      "conv.weight NÃO é um parametro treinável\n",
      "conv.bias é um parametro treinável. 256. torch.Size([256])\n",
      "Numero de parametros treinaveis ConvLora terceiro: 86656\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------Continuous Conv Lora NOSSO\n",
      "weight NÃO é um parametro treinável\n",
      "bias NÃO é um parametro treinável\n",
      "a é um parametro treinável. 384. torch.Size([1, 128, 3, 1])\n",
      "b é um parametro treinável. 384. torch.Size([1, 128, 1, 3])\n",
      "c é um parametro treinável. 32768. torch.Size([256, 128, 1, 1])\n",
      "d é um parametro treinável. 256. torch.Size([256])\n",
      "d_ é um parametro treinável. 1. torch.Size([1])\n",
      "Numero de parametros treinaveis no nosso ConvLora: 33793\n",
      "\n",
      "\n",
      "\n",
      "in_channel: 256 | out_channel 256\n",
      "weight é um parametro treinável. 589824. torch.Size([256, 256, 3, 3])\n",
      "bias é um parametro treinável. 256. torch.Size([256])\n",
      "Numero de parametros treinaveis na layer Conv normal: 590080\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------Continuous Conv Lora\n",
      "lora_A é um parametro treinável. 57600. torch.Size([75, 768])\n",
      "lora_B é um parametro treinável. 57600. torch.Size([768, 75])\n",
      "conv.weight NÃO é um parametro treinável\n",
      "conv.bias é um parametro treinável. 256. torch.Size([256])\n",
      "Numero de parametros treinaveis ConvLora terceiro: 115456\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------Continuous Conv Lora NOSSO\n",
      "weight NÃO é um parametro treinável\n",
      "bias NÃO é um parametro treinável\n",
      "a é um parametro treinável. 768. torch.Size([1, 256, 3, 1])\n",
      "b é um parametro treinável. 768. torch.Size([1, 256, 1, 3])\n",
      "c é um parametro treinável. 65536. torch.Size([256, 256, 1, 1])\n",
      "d é um parametro treinável. 256. torch.Size([256])\n",
      "d_ é um parametro treinável. 1. torch.Size([1])\n",
      "Numero de parametros treinaveis no nosso ConvLora: 67329\n",
      "\n",
      "\n",
      "\n",
      "in_channel: 256 | out_channel 512\n",
      "weight é um parametro treinável. 1179648. torch.Size([512, 256, 3, 3])\n",
      "bias é um parametro treinável. 512. torch.Size([512])\n",
      "Numero de parametros treinaveis na layer Conv normal: 1180160\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------Continuous Conv Lora\n",
      "lora_A é um parametro treinável. 57600. torch.Size([75, 768])\n",
      "lora_B é um parametro treinável. 115200. torch.Size([1536, 75])\n",
      "conv.weight NÃO é um parametro treinável\n",
      "conv.bias é um parametro treinável. 512. torch.Size([512])\n",
      "Numero de parametros treinaveis ConvLora terceiro: 173312\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------Continuous Conv Lora NOSSO\n",
      "weight NÃO é um parametro treinável\n",
      "bias NÃO é um parametro treinável\n",
      "a é um parametro treinável. 768. torch.Size([1, 256, 3, 1])\n",
      "b é um parametro treinável. 768. torch.Size([1, 256, 1, 3])\n",
      "c é um parametro treinável. 131072. torch.Size([512, 256, 1, 1])\n",
      "d é um parametro treinável. 512. torch.Size([512])\n",
      "d_ é um parametro treinável. 1. torch.Size([1])\n",
      "Numero de parametros treinaveis no nosso ConvLora: 133121\n",
      "\n",
      "\n",
      "\n",
      "in_channel: 512 | out_channel 512\n",
      "weight é um parametro treinável. 2359296. torch.Size([512, 512, 3, 3])\n",
      "bias é um parametro treinável. 512. torch.Size([512])\n",
      "Numero de parametros treinaveis na layer Conv normal: 2359808\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------Continuous Conv Lora\n",
      "lora_A é um parametro treinável. 115200. torch.Size([75, 1536])\n",
      "lora_B é um parametro treinável. 115200. torch.Size([1536, 75])\n",
      "conv.weight NÃO é um parametro treinável\n",
      "conv.bias é um parametro treinável. 512. torch.Size([512])\n",
      "Numero de parametros treinaveis ConvLora terceiro: 230912\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------Continuous Conv Lora NOSSO\n",
      "weight NÃO é um parametro treinável\n",
      "bias NÃO é um parametro treinável\n",
      "a é um parametro treinável. 1536. torch.Size([1, 512, 3, 1])\n",
      "b é um parametro treinável. 1536. torch.Size([1, 512, 1, 3])\n",
      "c é um parametro treinável. 262144. torch.Size([512, 512, 1, 1])\n",
      "d é um parametro treinável. 512. torch.Size([512])\n",
      "d_ é um parametro treinável. 1. torch.Size([1])\n",
      "Numero de parametros treinaveis no nosso ConvLora: 265729\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "from continuous_lora.layers.continuous_conv_lora_layer import ContinuousConvLoRALayer\n",
    "from continuous_lora.layers.old_conv_lora_layer import Conv2dLora\n",
    "\n",
    "\n",
    "\n",
    "in_channels=[3,64,64,128,128,256,256,512]\n",
    "out_channels=[64,64,128,128,256,256,512,512]\n",
    "\n",
    "for in_channel,out_channel in zip(in_channels,out_channels):\n",
    "    print(3*'\\n'+f'in_channel: {in_channel} | out_channel {out_channel}')\n",
    "    kernel_size=3\n",
    "    \n",
    "    r=25\n",
    "    \n",
    "    \n",
    "    normal_conv = nn.Conv2d(in_channels=in_channel, out_channels=out_channel, kernel_size=kernel_size)\n",
    "    normal_conv_num_params = 0\n",
    "    for name, p in normal_conv.named_parameters(): \n",
    "        if p.requires_grad:\n",
    "            normal_conv_num_params += p.numel()\n",
    "            print(f'{name} é um parametro treinável. {p.numel()}. {p.shape}')\n",
    "        else:\n",
    "            print(f'{name} NÃO é um parametro treinável')\n",
    "    \n",
    "    print(f'Numero de parametros treinaveis na layer Conv normal: {normal_conv_num_params}')\n",
    "    \n",
    "    print(3*'\\n'+40*'-'+'Continuous Conv Lora')\n",
    "    continuous_conv = ContinuousConvLoRALayer(in_channels=in_channel, out_channels=out_channel, kernel_size=kernel_size, number_of_tasks=1, conv_module=nn.Conv2d, r=r)\n",
    "    continuous_conv_num_params = 0\n",
    "    for name, p in continuous_conv.named_parameters(): \n",
    "        if p.requires_grad:\n",
    "            continuous_conv_num_params += p.numel()\n",
    "            print(f'{name} é um parametro treinável. {p.numel()}. {p.shape}')\n",
    "        else:\n",
    "            print(f'{name} NÃO é um parametro treinável')\n",
    "    \n",
    "    print(f'Numero de parametros treinaveis ConvLora terceiro: {continuous_conv_num_params}')\n",
    "    \n",
    "    print(3*'\\n'+40*'-'+'Continuous Conv Lora NOSSO')\n",
    "    continuous_conv_our = Conv2dLora(in_channels=in_channel, out_channels=out_channel, kernel_size=kernel_size)\n",
    "    continuous_conv_our.weight.requires_grad = False\n",
    "    continuous_conv_our.bias.requires_grad = False\n",
    "    continuous_conv_our_num_params = 0\n",
    "    for name, p in continuous_conv_our.named_parameters(): \n",
    "        if p.requires_grad:\n",
    "            continuous_conv_our_num_params += p.numel()\n",
    "            print(f'{name} é um parametro treinável. {p.numel()}. {p.shape}')\n",
    "        else:\n",
    "            print(f'{name} NÃO é um parametro treinável')\n",
    "    \n",
    "    print(f'Numero de parametros treinaveis no nosso ConvLora: {continuous_conv_our_num_params}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab7c6c02-ce40-44ae-8e4f-ef7eb0c41ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APENAS CONTANDO OS PARAMETROS DAS LAYERS CONVOLUCIONAIS. PRECISEI ALTERAR O CODIGO PARA NÃO CONGELAR AS LAYERS\n",
      "\n",
      "\n",
      "Número de parametros treinaveis no features extractor: 20024384\n",
      "Número de parametros treinaveis no classifier: 0\n",
      "Número de parametros treinaveis total: 20024384\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Adiciona o diretório pai ao caminho de pesquisa de módulos\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "\n",
    "from torch import nn\n",
    "from continuous_lora.models.lora_vgg19 import LoraVGG19\n",
    "import torchvision.models as models\n",
    "\n",
    "base_model = models.vgg19_bn(weights=\"IMAGENET1K_V1\")\n",
    "base_model.classifier[6] = nn.Linear(4096, 200)\n",
    "model = LoraVGG19(\n",
    "    model=base_model,\n",
    "    masks=[[i for i in range(200)]],\n",
    "    r_conv=1,\n",
    "    r_linear=100,\n",
    "    adapt_last_n_conv=0,\n",
    "    adapt_last_n_linear=0\n",
    ")\n",
    "print('APENAS CONTANDO OS PARAMETROS DAS LAYERS CONVOLUCIONAIS. PRECISEI ALTERAR O CODIGO PARA NÃO CONGELAR AS LAYERS\\n\\n')\n",
    "print(f'Número de parametros treinaveis no features extractor: {model.count_features_trainable_params()}')\n",
    "print(f'Número de parametros treinaveis no classifier: {model.count_classifier_trainable_params()}')\n",
    "print(f'Número de parametros treinaveis total: {model.count_trainable_params()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7aba7b78-f5ff-4308-86a5-a275493dfbd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APENAS CONTANDO OS PARAMETROS DOS FILTROS A,B,C e D (nosso). PRECISEI ALTERAR O CODIGO PARA USAR NOSSA IMPL.\n",
      "\n",
      "\n",
      "Número de parametros treinaveis no features extractor: 2259810\n",
      "Número de parametros treinaveis no classifier: 0\n",
      "Número de parametros treinaveis total: 2259810\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Adiciona o diretório pai ao caminho de pesquisa de módulos\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "\n",
    "from torch import nn\n",
    "from continuous_lora.models.lora_vgg19 import LoraVGG19\n",
    "import torchvision.models as models\n",
    "\n",
    "base_model = models.vgg19_bn(weights=\"IMAGENET1K_V1\")\n",
    "base_model.classifier[6] = nn.Linear(4096, 200)\n",
    "model = LoraVGG19(\n",
    "    model=base_model,\n",
    "    masks=[[i for i in range(200)]],\n",
    "    r_conv=1,\n",
    "    r_linear=100,\n",
    "    adapt_last_n_conv=16,\n",
    "    adapt_last_n_linear=0\n",
    ")\n",
    "\n",
    "print('APENAS CONTANDO OS PARAMETROS DOS FILTROS A,B,C e D (nosso). PRECISEI ALTERAR O CODIGO PARA USAR NOSSA IMPL.\\n\\n')\n",
    "print(f'Número de parametros treinaveis no features extractor: {model.count_features_trainable_params()}')\n",
    "print(f'Número de parametros treinaveis no classifier: {model.count_classifier_trainable_params()}')\n",
    "print(f'Número de parametros treinaveis total: {model.count_trainable_params()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cecbf2e-9e32-41e0-85d8-9dd047f9676a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APENAS CONTANDO OS PARAMETROS DOS FILTROS A,B (NOVA IMPLEMENTACAO) R=1. PRECISEI ALTERAR O CODIGO PARA USAR NOSSA IMPL.\n",
      "\n",
      "\n",
      "Número de parametros treinaveis no features extractor: 94491\n",
      "Número de parametros treinaveis no classifier: 0\n",
      "Número de parametros treinaveis total: 94491\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Adiciona o diretório pai ao caminho de pesquisa de módulos\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "\n",
    "from torch import nn\n",
    "from continuous_lora.models.lora_vgg19 import LoraVGG19\n",
    "import torchvision.models as models\n",
    "\n",
    "base_model = models.vgg19_bn(weights=\"IMAGENET1K_V1\")\n",
    "base_model.classifier[6] = nn.Linear(4096, 200)\n",
    "model = LoraVGG19(\n",
    "    model=base_model,\n",
    "    masks=[[i for i in range(200)]],\n",
    "    r_conv=1,\n",
    "    r_linear=100,\n",
    "    adapt_last_n_conv=16,\n",
    "    adapt_last_n_linear=0\n",
    ")\n",
    "\n",
    "print('APENAS CONTANDO OS PARAMETROS DOS FILTROS A,B (NOVA IMPLEMENTACAO) R=1. PRECISEI ALTERAR O CODIGO PARA USAR NOSSA IMPL.\\n\\n')\n",
    "print(f'Número de parametros treinaveis no features extractor: {model.count_features_trainable_params()}')\n",
    "print(f'Número de parametros treinaveis no classifier: {model.count_classifier_trainable_params()}')\n",
    "print(f'Número de parametros treinaveis total: {model.count_trainable_params()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ede4fcc-47be-4997-8fdc-7c49a2255dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APENAS CONTANDO OS PARAMETROS DOS FILTROS A,B (NOVA IMPLEMENTACAO) R=3. PRECISEI ALTERAR O CODIGO PARA USAR NOSSA IMPL.\n",
      "\n",
      "\n",
      "Número de parametros treinaveis no features extractor: 283473\n",
      "Número de parametros treinaveis no classifier: 0\n",
      "Número de parametros treinaveis total: 283473\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Adiciona o diretório pai ao caminho de pesquisa de módulos\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "\n",
    "from torch import nn\n",
    "from continuous_lora.models.lora_vgg19 import LoraVGG19\n",
    "import torchvision.models as models\n",
    "\n",
    "base_model = models.vgg19_bn(weights=\"IMAGENET1K_V1\")\n",
    "base_model.classifier[6] = nn.Linear(4096, 200)\n",
    "model = LoraVGG19(\n",
    "    model=base_model,\n",
    "    masks=[[i for i in range(200)]],\n",
    "    r_conv=3,\n",
    "    r_linear=100,\n",
    "    adapt_last_n_conv=16,\n",
    "    adapt_last_n_linear=0\n",
    ")\n",
    "\n",
    "print('APENAS CONTANDO OS PARAMETROS DOS FILTROS A,B (NOVA IMPLEMENTACAO) R=3. PRECISEI ALTERAR O CODIGO PARA USAR NOSSA IMPL.\\n\\n')\n",
    "print(f'Número de parametros treinaveis no features extractor: {model.count_features_trainable_params()}')\n",
    "print(f'Número de parametros treinaveis no classifier: {model.count_classifier_trainable_params()}')\n",
    "print(f'Número de parametros treinaveis total: {model.count_trainable_params()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14e02222-ee3d-4cf8-ba55-73a7f6c03a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APENAS CONTANDO OS PARAMETROS DOS FILTROS A,B (NOVA IMPLEMENTACAO) R=24. PRECISEI ALTERAR O CODIGO PARA USAR NOSSA IMPL.\n",
      "\n",
      "\n",
      "Número de parametros treinaveis no features extractor: 2267784\n",
      "Número de parametros treinaveis no classifier: 0\n",
      "Número de parametros treinaveis total: 2267784\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Adiciona o diretório pai ao caminho de pesquisa de módulos\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "\n",
    "from torch import nn\n",
    "from continuous_lora.models.lora_vgg19 import LoraVGG19\n",
    "import torchvision.models as models\n",
    "\n",
    "base_model = models.vgg19_bn(weights=\"IMAGENET1K_V1\")\n",
    "base_model.classifier[6] = nn.Linear(4096, 200)\n",
    "model = LoraVGG19(\n",
    "    model=base_model,\n",
    "    masks=[[i for i in range(200)]],\n",
    "    r_conv=24,\n",
    "    r_linear=100,\n",
    "    adapt_last_n_conv=16,\n",
    "    adapt_last_n_linear=0\n",
    ")\n",
    "\n",
    "print('APENAS CONTANDO OS PARAMETROS DOS FILTROS A,B (NOVA IMPLEMENTACAO) R=24. PRECISEI ALTERAR O CODIGO PARA USAR NOSSA IMPL.\\n\\n')\n",
    "print(f'Número de parametros treinaveis no features extractor: {model.count_features_trainable_params()}')\n",
    "print(f'Número de parametros treinaveis no classifier: {model.count_classifier_trainable_params()}')\n",
    "print(f'Número de parametros treinaveis total: {model.count_trainable_params()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e9ca0029-30bc-41fe-987f-05ec0ece871e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APENAS CONTANDO OS PARAMETROS DOS FILTROS A,B (NOVA IMPLEMENTACAO) R=12. PRECISEI ALTERAR O CODIGO PARA USAR NOSSA IMPL.\n",
      "\n",
      "\n",
      "Número de parametros treinaveis no features extractor: 1133892\n",
      "Número de parametros treinaveis no classifier: 0\n",
      "Número de parametros treinaveis total: 1133892\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Adiciona o diretório pai ao caminho de pesquisa de módulos\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "\n",
    "from torch import nn\n",
    "from continuous_lora.models.lora_vgg19 import LoraVGG19\n",
    "import torchvision.models as models\n",
    "\n",
    "base_model = models.vgg19_bn(weights=\"IMAGENET1K_V1\")\n",
    "base_model.classifier[6] = nn.Linear(4096, 200)\n",
    "model = LoraVGG19(\n",
    "    model=base_model,\n",
    "    masks=[[i for i in range(200)]],\n",
    "    r_conv=12,\n",
    "    r_linear=100,\n",
    "    adapt_last_n_conv=16,\n",
    "    adapt_last_n_linear=0\n",
    ")\n",
    "\n",
    "print('APENAS CONTANDO OS PARAMETROS DOS FILTROS A,B (NOVA IMPLEMENTACAO) R=12. PRECISEI ALTERAR O CODIGO PARA USAR NOSSA IMPL.\\n\\n')\n",
    "print(f'Número de parametros treinaveis no features extractor: {model.count_features_trainable_params()}')\n",
    "print(f'Número de parametros treinaveis no classifier: {model.count_classifier_trainable_params()}')\n",
    "print(f'Número de parametros treinaveis total: {model.count_trainable_params()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dba7195-feb0-436f-b45b-2128a72e2cf1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
